[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Oxford Nanopore Technology (ONT) Data",
    "section": "",
    "text": "This site has been temporarly designed to host analysis of ONT data.\nThis course website is still under development and should only be used for training purposes.\nClick here to access the course setup page or here to access the course materials and instructions."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Workshop Attendees\n\n\n\nIf you are attending one of our workshops, we will provide a training environment with all of the required software and data.\nIf you want to setup your own computer to run the analysis demonstrated on this course, you can follow the instructions below.\nNote that we use tabsets to provide instructions for all three major operating systems. However, as much as possible we advice you use a Linux system, as our training environment is built on that."
  },
  {
    "objectID": "setup.html#installing-conda",
    "href": "setup.html#installing-conda",
    "title": "Setup",
    "section": "Installing conda",
    "text": "Installing conda\nWe will perform a fresh installation of the conda package using the miniconda installation option.\n\n\n\n\n\n\nNote\n\n\n\nIf you already have Miniconda or Anaconda installed, and you just want to upgrade, you should not proceed to making a fresh installation. Just use conda update to update your existing version of conda.\nconda update conda\nAfter updatiing conda, you can proceed to the instructions from number 8 to install mamba into the base environment from the conda-forge channel.\n\n\n\nWindowsMac OSLinux\n\n\nFollow this link to install miniconda and this link to install mamba on your windows system.\nNB. Windows users can use WSL2 (see @wsl)\n\n\n\nOpen a terminal and follow the following instructions:\n\nNavigate to your home directory:\n\ncd ~\n\nDownload the Miniconda3 installer for mac by running:\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n\n\n\n\n\n\nM processor users\n\n\n\nFor M1 processor users, you will need to run the below command:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n\n\nRun the installation script just downloaded:\n\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\nFollow the installation instructions accepting default options (answering ‘yes’ to any questions)\n\n\nIf you are unsure about any setting, accept the defaults. You can change them later.\n\n\nTo make the changes take effect, close and then re-open your terminal window.\nTest your installation.\n\n\nIn your terminal window, run the command conda list:\n\nconda list\n\nA list of installed packages appears if it has been installed correctly.\n\n\nRemove the installation script as it is no longer needed if successfully installed:\n\nrm Miniconda3-latest-MacOSX-x86_64.sh\n\nRun the following command to add channels:\n\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda config --set channel_priority strict\nThis adds two channels (sources of software) useful for bioinformatics and data science applications.\n\nInstall Mamba into the base environment from the conda-forge channel with the below command:\n\nconda install mamba -n base -c conda-forge\n\nRun this to initiate mamba:\n\nmamba init\n\n\nOpen a terminal and follow the following instructions:\n\nNavigate to your home directory:\n\ncd ~\n\nDownload the Miniconda3 installer for Linux by running:\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\nRun the installation script just downloaded:\n\nbash Miniconda3-latest-Linux-x86_64.sh\n\nFollow the installation instructions accepting default options (answering ‘yes’ to any questions)\n\n\nIf you are unsure about any setting, accept the defaults. You can change them later.\n\n\nTo make the changes take effect, close and then re-open your terminal window.\nTest your installation.\n\n\nIn your terminal window, run the command conda list:\n\nconda list\n\nA list of installed packages appears if it has been installed correctly.\n\n\nRemove the installation script as it is no longer needed if successfully installed:\n\nrm Miniconda3-latest-Linux-x86_64.sh\n\nRun the following command to add channels:\n\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda config --set channel_priority strict\nThis adds two channels (sources of software) useful for bioinformatics and data science applications.\n\nInstall Mamba into the base environment from the conda-forge channel with the below command:\n\nconda install mamba -n base -c conda-forge\n\nRun this to initiate mamba:\n\nmamba init"
  },
  {
    "objectID": "setup.html#creating-conda-environments-for-the-workshop",
    "href": "setup.html#creating-conda-environments-for-the-workshop",
    "title": "Setup",
    "section": "Creating conda environments for the workshop",
    "text": "Creating conda environments for the workshop\n\nWindowsMac OSLinux\n\n\nContent will soon be uploaded.\n\n\n\n\n\n\n\n\n\ncreating the qc_ont environment and installing required packages\n\n\n\nOpen a terminal, make sure you are in the conda base environment and run this command to install all required packages and their dependencies:\nmamba create -n qc_ont fastq-scan=1.0.0 fastqc=0.11.9 fastp=0.23.2 porechop=0.2.4 kraken2=2.1.2 bracken=2.7 multiqc=1.13a\nThis creates an environment called qc_ont with the specified package versions and their dependencies.\nNB. The tools fastq-scan and bracken runs python scripts which require python libraries pandas, json, glob.\nUse the below command to install the packages in the qc_ont environment:\nconda install pandas=1.4.3 -n qc_ont -c conda-forge \n\n\n\n\n\n\n\n\ncreating the nanoplot environment and installing required packages\n\n\n\nOpen a terminal, make sure you are in the conda base environment and run this command to install all required packages and their dependencies:\nmamba create -n nanoplot nanoplot=1.41.6\nThis creates an environment called nanoplot with the specified package versions and their dependencies.\nNB. nanoplot depends on some packages that may conflict the tools in the qc_ont environment above, hence the need to create a seperate environment for it.\n\n\n\n\n\n\n\n\nInstalling required packages for [Assembly and Annotation]\n\n\n\nNB. For the Assembly and Annotation module, we will create several different environments, as many as the number of tools to be used. This is due to package conflicts. We are currently not able to place all the required tools in the same conda environment. .\nWe will thus, create each environment seperately with the following names:\nmamba create -n flye -c bioconda flye=2.9.2\nmamba create -n medaka medaka\nmamba create -n quast -c bioconda quast=5.2.0\nmamba create -n prokka -c bioconda prokka=1.14.6\n\n\n\n\n\n\n\n\nSpecify version of tool to install\n\n\n\nAs you may see, all the tools installed have specified version numbers added to the tool names in the format tool=version_numer. This allows us to install the exact version of tools used for the training.\nFor your personal use, if you wish to use the latest version of these tools, just omit specifying the version z-version_number` and the latest version of the tool will hopefully be installed.\n\n\n\n\n\n\n\n\n\n\ncreating the qc_ont environment and installing required packages\n\n\n\nOpen a terminal, make sure you are in the conda base environment and run this command to install all required packages and their dependencies:\nmamba create -n qc_ont fastq-scan=1.0.0 fastqc=0.11.9 fastp=0.23.2 porechop=0.2.4 kraken2=2.1.2 bracken=2.7 multiqc=1.13a\nThis creates an environment called qc_ont with the specified package versions and their dependencies.\nNB. The tools fastq-scan and bracken runs python scripts which require python libraries pandas, json, glob.\nUse the below command to install the packages in the qc_ont environment:\nconda install pandas -n qc_ont -c conda-forge \n\n\n\n\n\n\n\n\ncreating the nanoplot environment and installing required packages\n\n\n\nOpen a terminal, make sure you are in the conda base environment and run this command to install all required packages and their dependencies:\nmamba create -n nanoplot nanoplot=1.41.6\nThis creates an environment called nanoplot with the specified package versions and their dependencies.\nNB. nanoplot depends on some packages that may conflict the tools in the qc_ont environment above, hence the need to create a seperate environment for it.\n\n\n\n\n\n\n\n\nInstalling required packages for [Assembly and Annotation]\n\n\n\nNB. For the Assembly and Annotation module, we will create several different environments, as many as the number of tools to be used. This is due to package conflicts. We are currently not able to place all the required tools in the same conda environment. .\nWe will thus, create each environment seperately with the following names:\nmamba create -n flye -c bioconda flye=2.9.2\nmamba create -n medaka medaka\nmamba create -n quast -c bioconda quast=5.2.0\nmamba create -n prokka -c bioconda prokka=1.14.6\n\n\n\n\n\n\n\n\nSpecify version of tool to install\n\n\n\nAs you may see, all the tools installed have specified version numbers added to the tool names in the format tool=version_numer. This allows us to install the exact version of tools used for the training.\nFor your personal use, if you wish to use the latest version of these tools, just omit specifying the version z-version_number` and the latest version of the tool will hopefully be installed."
  },
  {
    "objectID": "setup.html#downloading-databases",
    "href": "setup.html#downloading-databases",
    "title": "Setup",
    "section": "Downloading databases",
    "text": "Downloading databases\n\n\n\n\n\n\n\nminikraken2 database\n\n\n\nDownload the kracken database “minikraken2_v1_8GB” into the database directory:\nwget ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v1_8GB_201904.tgz\nUncompress the database\ntar xvfz minikraken2_v1_8GB_201904.tgz \nIf the unzipped database is not same as the one use in the workshop, rename the it to match the workshop codes used using:\nmv <unzipped_database_name> minikraken2_v1_8GB\nYou can now remove the zipped downloaded database as it is no longer required\nrm minikraken2_v1_8GB_201904.tgz\n\n\n\n\n\n\n\n\nbakta database\n\n\n\nDownload the Bakta database “db.tar.gz” into the database directory and unzip.\n\n\n\n\n\n\none step\n\n\n\nIf you have the denove_assembly environment activated, you can perform this step.\nbakta_db download --output <output-path> \nIf you use this option, you don’t need to perform the AMRFinderPlus step as the AMR-DB will be included automatically.\n\n\nwget https://bakta-db.s3.computational.bio.uni-giessen.de/db.tar.gz\nor\nwget https://zenodo.org/record/7025248/files/db.tar.gz\nUncompress the database\ntar -xzf db.tar.gz\nRename the database to match the workshop codes used\nmv db bakta_db\nDelete zipped file after unzipping\nrm db.tar.gz\nDownload the AMRFinderPlus database\namrfinder_update --force_update --database bakta_db/amrfinderplus-db/\nUpdating an existing bakta database\nbakta_db update --db <existing-db-path> [--tmp-dir <tmp-directory>]\n\n\n\n\n\n\n\n\nseroba database\n\n\n\nFor git users, navigate to your database directory and clone the git repository:\ngit clone https://github.com/sanger-pathogens/seroba.git\nCopy the database from the seroba/ to your database directory — this should be your current directory:\ncp -r seroba/database .\nDelete the git repository to clean up your system:\nrm -r seroba\nStill in your database directory, rename the database to match the workshop codes used:\nmv database seroba_db"
  },
  {
    "objectID": "setup.html#nextflow",
    "href": "setup.html#nextflow",
    "title": "Setup",
    "section": "Nextflow",
    "text": "Nextflow\n\nWindowsMac OSLinux"
  },
  {
    "objectID": "setup.html#singularity",
    "href": "setup.html#singularity",
    "title": "Setup",
    "section": "Singularity",
    "text": "Singularity\n\nWindowsMac OSLinux\n\n\nYou can use Singularity from the Windows Subsystem for Linux (see @wsl).\nOnce you setup WSL, you can follow the instructions for Linux.\n\n\nSingularity is not available for Mac OS.\n\n\nThese instructions are for Ubuntu or Debian-based distributions1.\nsudo apt update && sudo apt upgrade && sudo apt install runc\nCODENAME=$(lsb_release -c | sed 's/Codename:\\t//')\nwget -O singularity.deb https://github.com/sylabs/singularity/releases/download/v3.10.2/singularity-ce_3.10.2-${CODENAME}_amd64.deb\nsudo dpkg -i singularity.deb\nrm singularity.deb"
  },
  {
    "objectID": "setup.html#visual-studio-code",
    "href": "setup.html#visual-studio-code",
    "title": "Setup",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\nWindowsMac OSLinux (Ubuntu)\n\n\n\nGo to the Visual Studio Code download page and download the installer for your operating system. Double-click the downloaded file to install the software, accepting all the default options.\nAfter completing the installation, go to your Windows Menu, search for “Visual Studio Code” and launch the application.\nGo to “File > Preferences > Settings”, then select “Text Editor > Files” on the drop-down menu on the left. Scroll down to the section named “EOL” and choose “\\n” (this will ensure that the files you edit on Windows are compatible with the Linux operating system).\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for Mac.\nGo to the Downloads folder and double-click the file you just downloaded to extract the application. Drag-and-drop the “Visual Studio Code” file to your “Applications” folder.\nYou can now open the installed application to check that it was installed successfully (the first time you launch the application you will get a warning that this is an application downloaded from the internet - you can go ahead and click “Open”).\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for your Linux distribution. Install the package using your system’s installer."
  },
  {
    "objectID": "setup.html#r-and-rstudio",
    "href": "setup.html#r-and-rstudio",
    "title": "Setup",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nWindowsMac OSLinux\n\n\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager."
  },
  {
    "objectID": "setup.html#workshop-data",
    "href": "setup.html#workshop-data",
    "title": "Setup",
    "section": "Workshop Data",
    "text": "Workshop Data"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "location of materials and sub-material\n\n\n\n\n\n\n\n\n← The materials are located on the left navigation pane\n\nYou can navigate to the various sub-material within each material in the right navigation pane→"
  },
  {
    "objectID": "materials/01-introduction/1.0-intro.html",
    "href": "materials/01-introduction/1.0-intro.html",
    "title": "1.0 Introduction to Oxford Nanopore Technology (ONT)",
    "section": "",
    "text": "This page is under construction."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html",
    "title": "2.1 Sequencing QC",
    "section": "",
    "text": "Teaching: 90 min || Exercises: 30 min"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#overview",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#overview",
    "title": "2.1 Sequencing QC",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow do you perform basic statistics to check the quality of Oxford Nanopore Technology (ONT) sequence reads?\nHow do you identify contaminants in your sequences?\nWhat are the various tools used for carrying out quality control?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nTo perform a Quality Control (QC) assessment of high throughput third generation sequence data - ONT\nInterpret and critically evaluate data quality reports\nTo identify possible contamination in high throughput sequence data\nRun simple scripts to achieve quality control of your genomic data\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nBefore analysing your nanopore sequencing data, it is critical to quality control the data and only carry on with genomes that pass quality control checks\nBest practice quality control assessments include:\n\nBase quality check\nIdentifying mismatches\nGC content and bias checks\nFalse insertions and deletions\nIdentifying contamination\netc.\n\nBest practice quality control tools for performing the quality checks include\n\nNanoPlot\nfastq-scan\nFastQC\nfastp\nPoreChop\nKraken 2\nBracken\nMultiQC\n\nOther tools which we will not explore here include\n\nNanofilt\nFiltlong\nminimap2\n\nIt is a good practice to clean up your directory as you proceed from analysis to analysis to avoid getting the error message that tells you you are running out of space/memory."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#background",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#background",
    "title": "2.1 Sequencing QC",
    "section": "2.1.1 Background",
    "text": "2.1.1 Background\nBefore we delve into having a look at our own genomic data. Lets take some minutes to explore what to look out for when performing Quality Control (QC) checks on our sequences. For this part of the course, we will largely focus on third generation sequences obtained from ONT sequencers. As you may already know from our previous lesson, the main output files expected from our ONT sequencer are .fast5 files which are readily converted to FastQ files for downstream analysis. Here, we will attempt to quality trim our Nanopore data using PoreChop and/or fastp and assess basic sequencing QC utilizing NanoPlot, fastq-scan and FastQC. Other frequently used QC tools include PycoQC. We would then check for contamination using both GC content statistics and Kraken 2. We will also combine reports from all these tools using MultiQC."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#joining-multiple-fastq-files",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#joining-multiple-fastq-files",
    "title": "2.1 Sequencing QC",
    "section": "2.1.2 Joining multiple FastQ files",
    "text": "2.1.2 Joining multiple FastQ files\nNormally, Guppy barcoder writes demultiplexed reads to multiple FastQ files per barcode, each having (normally) 4000 reads. To go further with your sequence data, you will have to join these FastQ files together into a single file per barcode.\nWe can do this in two ways with simple lines of commands.\n-1 Unzip gunzip the files, combine them into one zipped file gzip and redirect it > into the required file name.\nThe basic command looks like this.\ngunzip -c *.fastq.gz | gzip -c > joined.fastq.gz\nYou can now run the command on your first set of fastq files.\ngunzip -c *.fastq.gz | gzip -c > barcode30.fastq.gz\nThis uses a lot of memory and may not be the best option to perforn this task.\n-2 The second way to do this is to concatenate the files using the cat command.\ncat *.fastq.gz > barcode30.fastq.gz\nLet’s navigate into any of our ONT sequence directory and run any of the above codes. For instance let’s run the command on barcode30. Navigate to the barcode30 directory, copy paste the command and run it.\n\nQC assessment of ONT data\nAs you may already know, QC is an important part of any analysis. In this section we are going to look at some of the metrics and graphs that can be used to assess the QC of NGS data.\n\nBase quality\nIllumina sequencing technology relies on sequencing by synthesis. One of the most common problems with this is dephasing. For each sequencing cycle, there is a possibility that the replication machinery slips and either incorporates more than one nucleotide or perhaps misses to incorporate one at all. The more cycles that are run (i.e. the longer the read length gets), the greater the accumulation of these types of errors gets. This leads to a heterogeneous population in the cluster, and a decreased signal purity, which in turn reduces the precision of the base calling. The figure below shows an example of this.\n\n\n\nBase Quality\n\n\nBecause of dephasing, it is possible to have high-quality data at the beginning of the read but really low-quality data towards the end of the read. In those cases you can decide to trim off the low-quality reads, for example using a tool called Trimmomatic. In this workshop, we will do this using the tool fastp. In addition to trimming low quality reads, fastpwill also be used to trim off Illumina adapter/primer sequences.\nThe figures below shows an example of a high-quality read data (left) and a poor quality read data (right).\n\n\n\n\n\n\nHigh-quality read data\n\n\n\n\n\n\n\nPoor quality read data\n\n\n\n\n\n\nBase Quality Comparison\n\n\n\nIn addition to Phasing noise and signal decay resulting from dephasing issues described above, there are several different reasons for a base to be called incorrectly. You can lookup these later by clicking here.\n\n\nMismatches per cycle\nAligning reads to a high-quality reference genome can provide insight to the quality of a sequencing run by showing you the mismatches to the reference sequence. This can help you detect cycle-specific errors. Mismatches can occur due to two main causes, sequencing errors and differences between your sample and the reference genome, which is important to bear in mind when interpreting mismatch graphs. The figures below show an example of a good run (top) and a bad one (bottom). In the first figure, the distribution of the number of mismatches is even between the cycles, which is what we would expect from a good run. However, in the second figure, two cycles stand out with a lot of mismatches compared to the other cycles.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\n\n\nGC bias\nIt is a good idea to compare the GC content of the reads against the expected distribution in a reference sequence. The GC content varies between species, so a shift in GC content like the one seen below (right image) could be an indication of sample contamination. In the left image below, we can see that the GC content of the sample is about the same as for the theoretical reference, at ~65%. However, in the right figure, the GC content of the sample shows two distribution, one is closer to 40% and the other closer to 65%, indicating that there is an issue with this sample — a possible missed sample. Note that, suspecting contamination is perfectly fine for the species we are dealing with (MTBC). For other bacteria where there may be possibility of gene transfer, one can imagine that, such a situation may be from inheriting some plasmids that have a totally different GC content to the bacteria chromosome (This is arguable though).\n\n\n\n\n\n\nSingle GC distribution\n\n\n\n\n\n\n\nDouble GC distribution\n\n\n\n\n\n\nFigure: Base Quality Comparison\n\n\n\n\n\nGC content by cycle\nLooking at the GC content per cycle can help detect if the adapter sequence was trimmed. For a random library, it is expected to be little to no difference between the different bases of a sequence run, so the lines in this plot should be parallel with each other like in the first of the two figures below. In the second of the figures, the initial spikes are likely due to adapter sequences that have not been removed.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\n\n\nInsert size\nFor paired-end sequencing the size of DNA fragments also matters. In the first of the examples below, the insert size peaks around 440 bp. In the second however, there is also a peak at around 200 bp. This indicates that there was an issue with the fragment size selection during library prep.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\n\n\nInsertions/Deletions per cycle\nSometimes, air bubbles occur in the flow cell, which can manifest as false indels. The spike in the second image provides an example of how this can look.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\nIn addition to the QC plots you’ve encountered so far, there are other metrics that are generated with very powerful tools. For this workshop, we will explore these quality metrics with the help of fastq-scan and FastQC tools. It is often not a good practice to carry on analysis on samples that are contaminated with sequences from other species. We will identify contamination using either one of two ways. As earlier mentioned, the GC content varies between species, so a shift in GC content could be an indication of sample contamination. One other way of identifying sample contamination is by using specialized tools to determine/predict the species composition of your sample. For this course, we will determine species composition using the Kraken 2 database."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#navigate-to-the-ont-directory-and-activate-the-nanoplot-environment",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#navigate-to-the-ont-directory-and-activate-the-nanoplot-environment",
    "title": "2.1 Sequencing QC",
    "section": "2.1.3 Navigate to the ONT directory and activate the nanoplot environment",
    "text": "2.1.3 Navigate to the ONT directory and activate the nanoplot environment\nBefore we start, navigate into the ONT/ directory and activate the nanoplot environment:\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/ONT/\nmamba activate nanoplot"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#generating-qc-stats-and-metrics",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#generating-qc-stats-and-metrics",
    "title": "2.1 Sequencing QC",
    "section": "2.1.4 Generating QC stats and metrics",
    "text": "2.1.4 Generating QC stats and metrics\nWe are now ready to explore some quality metrics on our Nanopore sequence data.\n\n\n\n\n\n\nTools used and how to get help\n\n\n\nThis tutorial uses NanoPlot, fastq-scan, FastQC, fastp, PoreChop, Kraken 2, Bracken and MultiQC, which have been preinstalled for you in a virtual environment called qc_ont. This is to help speed up the pace of the workshop. However, NanoPlot is in a seperate environment due to dependency issues.\nIf for some reason, this environment is not pre-created for you, you can refer to the  page for creating this conda environment. This will also be a good practice for you. In a latter chapter of the course, you will also be introduced to how to set up these virtual environments and explore its usefulness. For each tool, to get help messages that describe how they are used, you can simply type the name of the tool and hit enter. This only works if you activate the environment in which the tools have already been installed. Alternatively, you can use the help flag -help or -h as appropriate. You can also invoke the manual of any tool (where available) by using the man command followed by the name of the tool.\n\n\n\nDisk Usage I — Before analysis\nBefore we start investigating our genomic sequences, let’s pause and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\n\n\n\nCurrent Disk Space In QC Directory\n\n~497M\n\nNow, keep this value in mind, we will come back to it at the end of the chapter.\n\n\nNanoPlot\nNanoplot is a plotting tool for long read sequencing data and alignments. It is also available as a web service.\nIn addition to various plots also a NanoStats file is created summarizing key features of the dataset.\nThis script performs data extraction from Oxford Nanopore sequencing data in the following formats:\n\nfastq files (can be bgzip, bzip2 or gzip compressed)\nfastq files generated by albacore, guppy or MinKNOW containing additional information (can be bgzip, bzip2 or gzip compressed)\nsorted bam files\nsequencing_summary.txt output table generated by albacore, guppy or MinKnow basecalling (can be gzip, bz2, zip and xz compressed)\nfasta files (can be bgzip, bzip2 or gzip compressed) Multiple files of the same type can be offered simultaneously\n\n\n\n\n\n\n\nHelp\n\n\n\nI have tried several options to pull out the help function of NanoPlot, only to find out that the N and P should be in capital letters.\nNanoPlot\nusage: NanoPlot [-h] [-v] [-t THREADS] [--verbose] [--store] [--raw] [--huge] [-o OUTDIR] [--no_static] [-p PREFIX]\n                [--tsv_stats] [--info_in_report] [--maxlength N] [--minlength N] [--drop_outliers] [--downsample N]\n                [--loglength] [--percentqual] [--alength] [--minqual N] [--runtime_until N] [--readtype {1D,2D,1D2}]\n                [--barcoded] [--no_supplementary] [-c COLOR] [-cm COLORMAP]\n                [-f [{png,jpg,jpeg,webp,svg,pdf,eps,json} ...]] [--plots [{kde,hex,dot} ...]]\n                [--legacy [{kde,dot,hex} ...]] [--listcolors] [--listcolormaps] [--no-N50] [--N50] [--title TITLE]\n                [--font_scale FONT_SCALE] [--dpi DPI] [--hide_stats]\n                (--fastq file [file ...] | --fasta file [file ...] | --fastq_rich file [file ...] | --fastq_minimal file [file ...] | --summary file [file ...] | --bam file [file ...] | --ubam file [file ...] | --cram file [file ...] | --pickle pickle | --feather file [file ...])\n\nCREATES VARIOUS PLOTS FOR LONG READ SEQUENCING DATA.\n\nGeneral options:\n  -h, --help            show the help and exit\n  -v, --version         Print version and exit.\n  -t, --threads THREADS\n                        Set the allowed number of threads to be used by the script\n  --verbose             Write log messages also to terminal.\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\nfor nanopore data\nNanoPlot\nusage: NanoPlot [-h] [-v] [-t THREADS] [--verbose] [--store] [--raw] [--huge] [-o OUTDIR] [--no_static] [-p PREFIX] [--tsv_stats] [--info_in_report] [--maxlength N]\n                [--minlength N] [--drop_outliers] [--downsample N] [--loglength] [--percentqual] [--alength] [--minqual N] [--runtime_until N] [--readtype {1D,2D,1D2}]\n                [--barcoded] [--no_supplementary] [-c COLOR] [-cm COLORMAP] [-f [{png,jpg,jpeg,webp,svg,pdf,eps,json} ...]] [--plots [{kde,hex,dot} ...]]\n                [--legacy [{kde,dot,hex} ...]] [--listcolors] [--listcolormaps] [--no-N50] [--N50] [--title TITLE] [--font_scale FONT_SCALE] [--dpi DPI] [--hide_stats]\n                (--fastq file [file ...] | --fasta file [file ...] | --fastq_rich file [file ...] | --fastq_minimal file [file ...] | --summary file [file ...] | --bam file [file ...] | --ubam file [file ...] | --cram file [file ...] | --pickle pickle | --feather file [file ...])\n\nCREATES VARIOUS PLOTS FOR LONG READ SEQUENCING DATA.\n\nGeneral options:\n  -h, --help            show the help and exit\n  -v, --version         Print version and exit.\n  -t, --threads THREADS\n                        Set the allowed number of threads to be used by the script\n  --verbose             Write log messages also to terminal.\n...\n\n\n\n\n\n\n\n\nOUTPUT\n\n\n\nNanoPlot creates:\n\na statistical summary\na number of plots\na html summary file\n\n\n\nNow run Nanoplot on your sample and interpret the html file produced.\nNanoPlot -t 2 --fastq barcode30.fastq.gz --maxlength 40000 --plots dot --legacy hex\n\n\n\n\n\n\n\nCopying files and directories with SCP or Rsync\n\n\n\nThe genral synthax for performing the above functions are:\n\nUsing scp\n\nTo copy a file from a remote server:\n\nscp ssh user@IP.address:/path/file_name /local/destination/path/\n\nTo copy a directory from a remote server:\n\nscp -r ssh user@IP.address:/path/directory[/] /local/destination/path/\n\nTo copy a file to a remote server:\n\nscp ssh /local/path/file_name user@IP.address:/destination/path/\n\nTo copy a directory to a remote server:\n\nscp -r ssh /local/path/directory[/]  user@IP.address:/destination/path/\n\n\nUsing rsync\nBecause Rsync transfers files recursively, you do not need to add the -r flag. You can use the following commands to transfer the files in an archived or compressed manner:\n\nTo copy a file from a remote server:\n\nrsync [-avz] user@IP.address:/path/file_name /local/destination/path/\n\nTo copy a directory from a remote server:\n\nrsync [-avz] user@IP.address:/path/directory[/] /local/destination/path/\n\nTo copy a file to a remote server:\n\nrsync [-avz] /local/path/file_name user@IP.address:/destination/path/\n\nTo copy a directory to a remote server:\n\nrsync [-avz] /local/path/directory[/]  user@IP.address:/destination/path/\nTo copy your html file from the server to your downloads, open a new terminal and cd to your downloads. Run the below command\nrsync -azhp student??@10.27.7.13:/home/student??/ONT/NanoPlot-report.html .\nscp -p ssh student??@10.27.7.13:/home/student??/ONT/NanoPlot-report.html .\n\n\n\nNow that we are done with NanoPlot, let’s deactivate the nanoplot environment and go ahead and activate the qc_ont environment, so we can run the remaining tools:\nmamba deactivate\nmamba activate qc_ont\n\n\nfastq-scan\nfastq-scan is a very quick tool that reads a FASTQ from STDIN and outputs summary statistics (read lengths, per-read qualities, per-base qualities) in JSON format.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for fastq-scan\nfastq-scan -h\nUsage: cat FASTQ | fastq-scan [options]\nVersion: 1.0.0\n\nOptional arguments:\n -g INT   Genome size for calculating estimated sequencing coverage. (Default 1)\n -p INT   ASCII offset for input quality scores, can be 33 or 64. (Default 33)  \n -q       Print only the QC stats, do not print read lengths or per-base quality scores\n -v       Print version information and exit\n -h       Show this message and exit\n\n\n\n\n\n\n\n\nUsage\n\n\n\nfastq-scan reads from STDIN, so pretty much any FASTQ output can be piped into fastq-scan. There are a few things to be aware of. It assumes that all FASTQ entries are the four line variant. Also, it has a PHRED offset (33 vs 64) guesser function. By default it is set to PHRED33, it could produce errors if there are not any PHRED33 or PHRED64 specific characters in the quality scores.\n\n\nYou can now go ahead and perform the fastq-scan with the below command.\n\n\n\n\n\n\nNote\n\n\n\nTry this command out first. You may have to add < after the zcat command depending on your OS. zcat works differently with different OS.\nDo this to run any of the ONT data and report on your findings:\nzcat barcode30.fastq.gz | fastq-scan -g 4000000 > barcode30_fastq-scan.json\n*NB. In the above command, we assume a genome size of 4Mb.\n\n\nYou should now see one new file generated in your current directory.\nTo see if the file have been created, use this command:\n\n\n\n\n\n\nls | grep \"fastq-scan.json\"\nbarcode30_fastq-scan.json\n\n\n\nTo view the statistics for your data use this command:\ncat barcode30_fastq-scan.json\n\n\n\n\n\n\nNote\n\n\n\nThe file created is rather small, so you can afford to use cat to view the entire content. You may want to use head or less for huge files.\n\n\nAs you may have realized, the content of the output file doesn’t look friendly.\nLet’s convert the .json files into a more friendly format, say tsv. You should know what tsv files are by now. If not, you can visite this link to explore more on file-formats to equip yourself.\n\nParse json files into tsv format\nWe will run a simple python script to achieve this purpose. In your working directory, you will find a file named fastq-scan_parser.py. This is a simple python script that will do the job for us.\nWe can go ahead and execute that python script by running the command below:\npython fastq-scan_parser.py\nYou should now see another file generated in your directory called fastq-scan_summary.tsv. What the python script did was to extract the relevant information from all .json files in the working directory and convert them to a .tsv file.\nNow let’s go ahead and have a look at the .tsv file with the command\ncat fastq-scan_summary.tsv\nsample  total_bp    coverage    read_total  read_min    read_mean   read_std    read_median read_max    read_25th   read_75th   qual_min    qual_mean   qual_std    qual_max    qual_median qual_25th   qual_75th\nbarcode30   305515607   71.0501 3024907 101 101.0   0.0 101 101 101 101 14  35.2243 3.49844 37  37  35  37\n\nAlternatively, you can open the tsv file with any appropriate GUI software (excel or libreoffice)\n\n\n\nFastQC\nFastQC is a program designed to spot potential problems in high throughput sequencing datasets. It runs a set of analyses on one or more raw sequence files in fastq or bam format and produces a report which summarises the results. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.\nFastQC will highlight any areas where the library looks unusual and where you should take a closer look. The program is not tied to any specific type of sequencing technique and can be used to look at libraries coming from a large number of different experiment types (Genomic Sequencing, ChIP-Seq, RNA-Seq, BS-Seq etc etc).\nThe main functions of FastQC are:\n\nImport of data from BAM, SAM or FastQ files (any variant)\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your data\nExport of results to an HTML based permanent report\nOffline operation to allow automated generation of reports without running the interactive application\n\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for FastQC\nfastqc -h\n\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n    FastQC reads a set of sequence files and produces from each one a quality\n    control report consisting of a number of different modules, each one of \n    which will help to identify a different potential type of problem in your\n    data.\n    \n    If no files to process are specified on the command line then the program\n    will start as an interactive graphical application.  If files are provided\n    on the command line then the program will run with no user interaction\n    required.  In this mode it is suitable for inclusion into a standardised\n    analysis pipeline.\n    \n    The options for the program as as follows:\n    \n    -h --help       Print this help file and exit\n    \n    -v --version    Print the version of the program and exit\n    \n    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n                    \n    --casava        Files come from raw casava output. Files in the same sample\n                    group (differing only by the group number) will be analysed\n                    as a set rather than individually. Sequences with the filter\n                    flag set in the header will be excluded from the analysis.\n                    Files must have the same names given to them by casava\n                    (including being gzipped and ending with .gz) otherwise they\n                    won't be grouped together correctly.\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nFastQC reads a set of sequence files and produces from each one a quality control report consisting of a number of different modules, each one of which will help to identify a different potential type of problem in your data. If no files to process are specified on the command line then the program will start as an interactive graphical application. If files are provided on the command line then the program will run with no user interaction required. In this mode it is suitable for inclusion into a standardised analysis pipeline.\nThe general format of the command is:\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 seqfile2 .. seqfileN\n\n\nYou can now go ahead and perform the fastQC with the below command.\n\n\n\n\n\n\nNote\n\n\n\nDo this to run your Nanopore data:\nfastqc --threads 4 barcode30.fastq.gz\nStarted analysis of barcode30.fastq.gz\nApprox 5% complete for barcode30.fastq.gz\nApprox 10% complete for barcode30.fastq.gz\nApprox 15% complete for barcode30.fastq.gz\nApprox 20% complete for barcode30.fastq.gz\nApprox 25% complete for barcode30.fastq.gz\n...\n\n\n\n\n\n\n\n\nOutput\n\n\n\nIf you specified an output -o directory then you should look out for that file being created in that directory. For our situation, we didn’t specify any output directory so the result will just be in the current directory. You should now see two new files generated in your current directory.\n\n\nHow do you tell which file has been recently produced?\n\n\nHint\n\nPerform a simple ls command with the arguments -lhrt and the last file in the output should be the most recent.\nls -lhrt\n\nWe are interested in the one that ends with .html. Go ahead and open it. Being an .html file, it will prefer to open in a browser, and that’s just how we want to make sense out of it.\nWe have already seen some of the content of the output file from the background to this chapter. However, this time, let’s look at a few more and also with some more details.\n\nBasic Statistics\nThe first information you encounter is the basic statistics. The Basic Statistics module generates some simple composition statistics for the file analysed.\n\nFilename: The original filename of the file which was analysed\nFile type: Says whether the file appeared to contain actual base calls or colorspace data which had to be converted to base calls\nEncoding: Says which ASCII encoding of quality values was found in this file.\nTotal Sequences: A count of the total number of sequences processed. There are two values reported, actual and estimated. At the moment these will always be the same.\nFiltered Sequences: If running in Casava mode sequences flagged to be filtered will be removed from all analyses. The number of such sequences removed will be reported here. The total sequences count above will not include these filtered sequences and will the number of sequences actually used for the rest of the analysis.\nSequence Length: Provides the length of the shortest and longest sequence in the set. If all sequences are the same length only one value is reported.\n%GC: The overall %GC of all bases in all sequences\n\n\n\n\nFastQC Basic Statistics\n\n\n\n\nPer Base Sequence Quality\nThe plot shows an overview of the range of quality values across all bases at each position in the FastQ file.\n\n\n\nPer Base Sequence Quality\n\n\nFor each position a BoxWhisker type plot is drawn. The elements of the plot are as follows: - The central red line is the median value - The yellow box represents the inter-quartile range (25-75%) - The upper and lower whiskers represent the 10% and 90% points - The blue line represents the mean quality\nThe y-axis on the graph shows the quality scores. The higher the score the better the base call. The background of the graph divides the y-axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). The quality of calls on most platforms will degrade as the run progresses, so it is common to see base calls falling into the orange area towards the end of a read.\nIt should be mentioned that there are number of different ways to encode a quality score in a FastQ file. FastQC attempts to automatically determine which encoding method was used, but in some very limited datasets it is possible that it will guess this incorrectly (ironically only when your data is universally very good!). The title of the graph will describe the encoding FastQC thinks your file used.\nNB. Results from this module will not be displayed if your input is a BAM/SAM file in which quality scores have not been recorded.\nWarning A warning will be issued if the lower quartile for any base is less than 10, or if the median for any base is less than 25.\nFailure This module will raise a failure if the lower quartile for any base is less than 5 or if the median for any base is less than 20.\n\n\nCommon reasons for warnings\n\nThe most common reason for warnings and failures in this module is a general degradation of quality over the duration of long runs. In general sequencing chemistry degrades with increasing read length and for long runs you may find that the general quality of the run falls to a level where a warning or error is triggered.\nIf the quality of the library falls to a low level then the most common remedy is to perform quality trimming where reads are truncated based on their average quality. For most libraries where this type of degradation has occurred you will often be simultaneously running into the issue of adapter read-through so a combined adapter and quality trimming step is often employed.\nAnother possibility is that a warn / error is triggered because of a short loss of quality earlier in the run, which then recovers to produce later good quality sequence. This can happen if there is a transient problem with the run (bubbles passing through a flow cell for example). You can normally see this type of error by looking at the per-tile quality plot (if available for your platform). In these cases trimming is not advisable as it will remove later good sequence, but you might want to consider masking bases during subsequent mapping or assembly.\nIf your library has reads of varying length then you can find a warning or error is triggered from this module because of very low coverage for a given base range. Before committing to any action, check how many sequences were responsible for triggering an error by looking at the sequence length distribution module results.\n\n\n\nPer Sequence Quality Scores\nThe per sequence quality score report allows you to see if a subset of your sequences have universally low quality values. It is often the case that a subset of sequences will have universally poor quality, often because they are poorly imaged (on the edge of the field of view etc), however these should represent only a small percentage of the total sequences.\n\n\n\nPer Sequence Quality Scores\n\n\nIf a significant proportion of the sequences in a run have overall low quality then this could indicate some kind of systematic problem - possibly with just part of the run (for example one end of a flow cell).\nNB. Results from this module will not be displayed if your input is a BAM/SAM file in which quality scores have not been recorded.\nWarning A warning is raised if the most frequently observed mean quality is below 27 - this equates to a 0.2% error rate.\nFailure An error is raised if the most frequently observed mean quality is below 20 - this equates to a 1% error rate.\n\n\nCommon reasons for warnings\n\nThis module is generally fairly robust and errors here usually indicate a general loss of quality within a run. For long runs this may be alleviated through quality trimming. If a bi-modal, or complex distribution is seen then the results should be evaluated in concert with the per-tile qualities (if available) since this might indicate the reason for the loss in quality of a subset of sequences.\n\n\n\nPer Base Sequence Content\nPer Base Sequence Content plots out the proportion of each base position in a file for which each of the four normal DNA bases has been called.\n\n\n\nPer Base Sequence Content\n\n\nIn a random library you would expect that there would be little to no difference between the different bases of a sequence run, so the lines in this plot should run parallel with each other. The relative amount of each base should reflect the overall amount of these bases in your genome, but in any case they should not be hugely imbalanced from each other.\nIt’s worth noting that some types of library will always produce biased sequence composition, normally at the start of the read. Libraries produced by priming using random hexamers (including nearly all RNA-Seq libraries) and those which were fragmented using transposases inherit an intrinsic bias in the positions at which reads start. This bias does not concern an absolute sequence, but instead provides enrichment of a number of different K-mers at the 5’ end of the reads. Whilst this is a true technical bias, it isn’t something which can be corrected by trimming and in most cases doesn’t seem to adversely affect the downstream analysis. It will however produce a warning or error in this module.\nWarning This module issues a warning if the difference between A and T, or G and C is greater than 10% in any position.\nFailure This module will fail if the difference between A and T, or G and C is greater than 20% in any position.\n\n\nCommon reasons for warnings\n\nThere are a number of common scenarios which would elicit a warning or error from this module.\nOverrepresented sequences: If there is any evidence of overrepresented sequences such as adapter dimers or rRNA in a sample then these sequences may bias the overall composition and their sequence will emerge from this plot. Biased fragmentation: Any library which is generated based on the ligation of random hexamers or through tagmentation should theoretically have good diversity through the sequence, but experience has shown that these libraries always have a selection bias in around the first 12bp of each run. This is due to a biased selection of random primers, but doesn’t represent any individually biased sequences. Nearly all RNA-Seq libraries will fail this module because of this bias, but this is not a problem which can be fixed by processing, and it doesn’t seem to adversely affect the ability to measure expression. Biased composition libraries: Some libraries are inherently biased in their sequence composition. The most obvious example would be a library which has been treated with sodium bisulphite which will then have converted most of the cytosines to thymines, meaning that the base composition will be almost devoid of cytosines and will thus trigger an error, despite this being entirely normal for that type of library If you are analysing a library which has been aggressively adapter trimmed then you will naturally introduce a composition bias at the end of the reads as sequences which happen to match short stretches of adapter are removed, leaving only sequences which do not match. Sudden deviations in composition at the end of libraries which have undergone aggressive trimming are therefore likely to be spurious.\n\n\n\n[Per Sequence GC content]\nWe will talk more about GC content under identifying contaminiation. For now, let’s just have a look at our plot.\n\n\n\nPer_sequence_GC Content\n\n\n\n\nPer Base N Content\nIf a sequencer is unable to make a base call with sufficient confidence then it will normally substitute an N rather than a conventional base] call\nThis module plots out the percentage of base calls at each position for which an N was called.\n\n\n\nPer Base N Content\n\n\nIt’s not unusual to see a very low proportion of Ns appearing in a sequence, especially nearer the end of a sequence. However, if this proportion rises above a few percent it suggests that the analysis pipeline was unable to interpret the data well enough to make valid base calls.\nWarning This module raises a warning if any position shows an N content of >5%.\nFailure This module will raise an error if any position shows an N content of >20%.\n\n\nCommon reasons for warnings\n\nThe most common reason for the inclusion of significant proportions of Ns is a general loss of quality, so the results of this module should be evaluated in concert with those of the various quality modules. You should check the coverage of a specific bin, since it’s possible that the last bin in this analysis could contain very few sequences, and an error could be prematurely triggered in this case.\nAnother common scenario is the incidence of a high proportions of N at a small number of positions early in the library, against a background of generally good quality. Such deviations can occur when you have very biased sequence composition in the library to the point that base callers can become confused and make poor calls. This type of problem will be apparent when looking at the per-base sequence content results.\n\nI am sure by now, you will be bored with the many quality control tools. Now, let’s try our hands on some trimming to improve the quality of our sequences. To do this, we will use fastpand PoreChop.\n\n\n\nfastp\nclick here to view the publication on fastp fastp is a tool designed to provide fast all-in-one pre-processing for FastQ files.\nIt is mostly used on short-reads illumina data ban can be applied to long-read ONT.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for fastp\nfastp --help\nfastp: an ultra-fast all-in-one FASTQ pre-processor\nversion 0.23.2\nusage: fastp [options] ... \noptions:\n  -i, --in1                            read1 input file name (string [=])\n  -o, --out1                           read1 output file name (string [=])\n  -I, --in2                            read2 input file name (string [=])\n  -O, --out2                           read2 output file name (string [=])\n      --unpaired1                      for PE input, if read1 passed QC but read2 not, it will be written to unpaired1. Default is to discard it. (string [=])\n      --unpaired2                      for PE input, if read2 passed QC but read1 not, it will be written to unpaired2. If --unpaired2 is same as --unpaired1 (default mode), both unpaired reads will be written to this same file. (string [=])\n...\n\n\n\nFeatures\n\ncomprehensive quality profiling for both before and after filtering data (quality - curves, base contents, KMER, Q20/Q30, GC Ratio, duplication, adapter contents…)\nfilter out bad reads (too low quality, too short, or too many N…)\ncut low quality bases for per read in its 5’ and 3’ by evaluating the mean quality - from a sliding window (like Trimmomatic but faster).\ntrim all reads in front and tail\ncut adapters. Adapter sequences can be automatically detected, which means you don’t have to input the adapter sequences to trim them.\ncorrect mismatched base pairs in overlapped regions of paired end reads, if one base is with high quality while the other is with ultra low quality\ntrim polyG in 3’ ends, which is commonly seen in NovaSeq/NextSeq data. Trim polyX in 3’ ends to remove unwanted polyX tailing (i.e. polyA tailing for mRNA-Seq data)\npreprocess unique molecular identifier (UMI) enabled data, shift UMI to sequence name.\nreport JSON format result for further interpreting.\nvisualize quality control and filtering results on a single HTML page (like FASTQC - but faster and more informative).\nsplit the output to multiple files (0001.R1.gz, 0002.R1.gz…) to support parallel - processing. Two modes can be used, limiting the total split file number, or limiting the lines of each split file.\nsupport long reads (data from PacBio / Nanopore devices).\nsupport reading from STDIN and writing to STDOUT\nsupport interleaved input\nsupport ultra-fast FASTQ-level deduplication\n\n\n\n\n\n\n\nUsage\n\n\n\nfor single end data (not compressed)\nfastp -i in.fq -o out.fq\nfor paired end data (gzip compressed)\nfastp -i in_R1.fq.gz -I in_R2.fq.gz -o out_R1.fq.gz -O out_R2.fq.gz\nBy default, the HTML report is saved to fastp.html (can be specified with -h option), and the JSON report is saved to fastp.json (can be specified with -j option).\n\n\nYou can now go ahead and perform the fastp with the below command. We will perform the run on one of our Nanopore reads.\n\n\n\n\n\n\nNote\n\n\n\nfastp --cut_front --cut_tail --trim_poly_x --cut_mean_quality 10 --qualified_quality_phred 10 --unqualified_percent_limit 10 --length_required 50 --in1 barcode30.fastq.gz --out1 barcode30.trim.fastq.gz --json barcode30_fastp.json --html barcode30_fastp.html --thread 4\nDetecting adapter sequence for read1...\nNo adapter detected for read1\n\nRead1 before filtering:\ntotal reads: 54368\ntotal bases: 257262215\nQ20 bases: 140726297(54.7015%)\nQ30 bases: 59979993(23.3147%)\n\nRead1 after filtering:\ntotal reads: 2788\ntotal bases: 13508368\nQ20 bases: 9981247(73.8894%)\nQ30 bases: 4923721(36.4494%)\n\nFiltering result:\nreads passed filter: 2788\nreads failed due to low quality: 51575\nreads failed due to too many N: 0\nreads failed due to too short: 5\nreads with adapter trimmed: 0\nbases trimmed due to adapters: 0\nreads with polyX in 3' end: 142\nbases trimmed in polyX tail: 4340\n\nDuplication rate (may be overestimated since this is SE data): 0%\n\nJSON report: barcode30_fastp.json\nHTML report: barcode30_fastp.html\n\nfastp --cut_front --cut_tail --trim_poly_x --cut_mean_quality 10 --qualified_quality_phred 10 --unqualified_percent_limit 10 --length_required 50 --in1 barcode30.fastq.gz --out1 barcode30.trim.fastq.gz --json barcode30_fastp.json --html barcode30_fastp.html --thread 4\nfastp v0.23.2, time used: 5 seconds\n\n...\nYou can also read out what is printed out on the terminal as a log file by passing the output to a textfile named fastp.log. This way, you don’t see all the long print out on the terminal.\nfastp --cut_front --cut_tail --trim_poly_x --cut_mean_quality 10 --qualified_quality_phred 10 --unqualified_percent_limit 10 --length_required 50 --in1 barcode30.fastq.gz --out1 barcode30.trim.fastq.gz --json barcode30_fastp.json --html barcode30_fastp.html --thread 4 > fastp.log\n\n\nNote that the analysis perfomed above is just for demonstration purposes hence parameters set may not represent the true states.\nYou should now see three new files generated in your current directory.\nbarcode30.trim.fastq.gz\nbarcode30_fastp.json\nbarcode30_fastp.html\nNow let’s go ahead and see what our output looks like by opening the barcode30_fastp.html file. You should be able to interpret the output by now.\n\n\n\n\n\n\nNote\n\n\n\nNote that the date presented below are just for demonstration purposes adn do not represent the actual data from the barcode30.fastq.gz reads.\n\n\n\n\n\nSummary of fastp report\n\n\nLet’s have a look at the quality of the reads before and after trimming. What obvious differences do you notice?\n\n\n\n\n\n\nRead Quality before filtering\n\n\n\n\n\n\n\nRead Quality after filtering\n\n\n\n\n\n\nRead Quality before and after fastp processing\n\n\n\n\n\n\n\n\n\nBase content quality before filtering\n\n\n\n\n\n\n\nBase content quality after filtering\n\n\n\n\n\n\nBase Content Quality before and after fastp processing\n\n\n\n\n\n\n\nPoreChop\n\nPorechop is a tool for finding and removing adapters from Oxford Nanopore reads. Adapters on the ends of reads are trimmed off, and when a read has an adapter in its middle, it is treated as chimeric and chopped into separate reads. Porechop performs thorough alignments to effectively find adapters, even at low sequence identity.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for PoreChop\nporechop --help\nusage: porechop -i INPUT [-o OUTPUT] [--format {auto,fasta,fastq,fasta.gz,fastq.gz}] [-v VERBOSITY] [-t THREADS] [-b BARCODE_DIR]\n                [--barcode_threshold BARCODE_THRESHOLD] [--barcode_diff BARCODE_DIFF] [--require_two_barcodes] [--untrimmed]\n                [--discard_unassigned] [--adapter_threshold ADAPTER_THRESHOLD] [--check_reads CHECK_READS] [--scoring_scheme SCORING_SCHEME]\n                [--end_size END_SIZE] [--min_trim_size MIN_TRIM_SIZE] [--extra_end_trim EXTRA_END_TRIM] [--end_threshold END_THRESHOLD]\n                [--no_split] [--discard_middle] [--middle_threshold MIDDLE_THRESHOLD]\n                [--extra_middle_trim_good_side EXTRA_MIDDLE_TRIM_GOOD_SIDE] [--extra_middle_trim_bad_side EXTRA_MIDDLE_TRIM_BAD_SIDE]\n                [--min_split_read_size MIN_SPLIT_READ_SIZE] [-h] [--version]\n\nPorechop: a tool for finding adapters in Oxford Nanopore reads, trimming them from the ends and splitting reads with internal adapters\n\nMain options:\n  -i INPUT, --input INPUT               FASTA/FASTQ of input reads or a directory which will be recursively searched for FASTQ files\n                                        (required)\n  -o OUTPUT, --output OUTPUT            Filename for FASTA or FASTQ of trimmed reads (if not set, trimmed reads will be printed to stdout)\n  --format {auto,fasta,fastq,fasta.gz,fastq.gz}\n                                        Output format for the reads - if auto, the format will be chosen based on the output filename or the\n                                        input read format (default: auto)\n  -v VERBOSITY, --verbosity VERBOSITY   Level of progress information: 0 = none, 1 = some, 2 = lots, 3 = full - output will go to stdout if\n                                        reads are saved to a file and stderr if reads are printed to stdout (default: 1)\n  -t THREADS, --threads THREADS         Number of threads to use for adapter alignment (default: 4)\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nFor basic adapter trimming:\nporechop -i input_reads.fastq.gz -o output_reads.fastq.gz\n\n\n\nHow it works\nFind matching adapter sets\nPorechop first aligns a subset of reads (default 10000 reads, change with --check_reads) to all known adapter sets. Adapter sets with at least one high identity match (default 90%, change with --adapter_threshold) are deemed present in the sample.\nIdentity in this step is measured over the full length of the adapter. E.g. in order to qualify for a 90% match, an adapter could be present at 90% identity over its full length, or it could be present at 100% identity over 90% of its length, but a 90% identity match over 90% of the adapter length would not be sufficient.\nThe alignment scoring scheme used in this and subsequent alignments can be modified using the --scoring_scheme option (default: match = 3, mismatch = -6, gap open = -5, gap extend = -2).\nTrim adapters from read ends\nThe first and last bases in each read (default 150 bases, change with --end_size) are aligned to each present adapter set. When a long enough (default 4, change with --min_trim_size) and strong enough (default 75%, change with –end_threshold) match is found, the read is trimmed. A few extra bases (default 2, change with --extra_end_trim) past the adapter match are removed as well to ensure it’s all removed.\nIdentity in this step is measured over the aligned part of the adapter, not its full length. E.g. if the last 5 bases of an adapter exactly match the first 5 bases of a read, that counts as a 100% identity match and those bases will be trimmed off. This allows Porechop to effectively trim partially present barcodes.\nThe default --end_threshold is low (75%) because false positives (trimming off some sequence that wasn’t really an adapter) shouldn’t be too much of a problem with long reads, as only a tiny fraction of the read is lost.\nSplit reads with internal adapters\nThe entirety of each read is aligned to the present adapter sets to spot cases where an adapter is in the middle of the read, indicating a chimera. When a strong enough match is found (default 85%, change with --middle_threshold), the read is split. If the resulting parts are too short (default less than 1000 bp, change with --min_split_read_size), they are discarded.\nThe default --middle_threshold (85%) is higher than the default --end_threshold (75%) because false positives in this step (splitting a read that is not chimeric) could be more problematic than false positives in the end trimming step. If false negatives (failing to split a chimera) are worse for you than false positives (splitting a non-chimera), you should reduce this threshold (e.g. --middle_threshold 75).\nExtra bases are also removed next to the hit, and how many depends on the side of the adapter. If we find an adapter that’s expected at the start of a read, it’s likely that what follows is good sequence but what precedes it may not be. Therefore, a few bases are trimmed after the adapter (default 10, change with --extra_middle_trim_good_side) and more bases are trimmed before the adapter (default 100, change with --extra_middle_trim_bad_side). If the found adapter is one we’d expect at the end of the read, then the “good side” is before the adapter and the “bad side” is after the adapter.\nHere is a real example of the “good” and “bad” sides of an adapter. The adapter is in the middle of this snippet (SQK-NSK007_Y_Top at about 90% identity). The bases to the left are the “bad” side and their repetitive nature is clear. The bases to the right are the “good” side and represent real biological sequence.\nTGTTGTTGTTGTTATTGTTGTTATTGTTGTTGTATTGTTGTTATTGTTGTTGTTGTACATTGTTATTGTTGTATTGTTGTTATTGTTGTTGTATTATCGGTGTACTTCGTTCAGTTACGTATTACTATCGCTATTGTTTGCAGTGAGAGGTGGCGGTGAGCGTTTTCAAATGGCCCTGTACAATCATGGGATAACAACATAAGGAACGGACCATGAAGTCACTTCT\nDiscard reads with internal adapters\nIf you run Porechop with --discard_middle, the reads with internal adapters will be thrown out instead of split.\nIf you plan on using your reads with Nanopolish, then the --discard_middle option is required. This is because Nanopolish first runs nanopolish index to find a one-to-one association between FASTQ reads and fast5 files. If you ran Porechop without --discard_middle, then you could end up with multiple separate FASTQ reads which are from a single fast5, and this is incompatible with Nanopolish.\nThis option is also recommended if you are trimming reads from a demultiplexed barcoded sequencing run. This is because chimeric reads may contain two sequences from two separate barcodes, so throwing them out is the safer option to reduce cross-barcode contamination.\nPorechop can perform a lot of other very important analysis including demultiplexing. You can find more by following this link.\nFor now let’s make our hands dry by performing porechop on any of our reads.\n\n\n\n\n\n\nporechop -i barcode30.fastq.gz -o barcode30_porechopped.fastq.gz\nLoading reads\nbarcode30.fastq.gz\n54,368 reads loaded\n\n\nLooking for known adapter sets\n10,000 / 10,000 (100.0%)\n                                        Best               \n                                        read       Best    \n                                        start      read end\n  Set                                   %ID        %ID     \n  SQK-NSK007                                82.1       80.0\n  Rapid                                    100.0        0.0\n  RBK004_upstream                           86.8        0.0\n  SQK-MAP006                                78.6       80.0\n...\n\n\n\nWe see from our run, three basic analysis:\n\nTrimming adapter from the reads\nSplitting reads containing adapter\nsaving trimed reads to file\n\n\n\n\nPorechop basic analysis\n\n\nAlso take time to go throught the list of adapters. Do you see anything unusual?\n\n\nHint\n\nYou will find that only the adapters used for your samples are highlighted\n\n\n\nKnown issues\nAdapter search\nPorechop tries to automatically determine which adapters are present by looking at the reads, but this approach has a few issues:\n\nAs the number of kits/barcodes has grown, adapter-search as part of the Porechop’s pipeline has become increasingly slow.\nPorechop only does the adapter search on a subset of reads, which means there can be problems with non-randomly ordered read sets (e.g. all barcode 1 reads at the start of a file, followed by barcode 2 reads, etc).\nMany ONT adapters share common sequence with each other, making false positive adapter finds possible.\n\n\n\n\n\n\n\nExercise 2.1.1: Going back to run fastqc and fastp\n\n\n\nNow that you are almost becoming a pro in the use fo the commandline, - write down the code to run fastqc and fastp on your output from the previous run. In this case our trimmed reads - barcode30_porechopped.fastq.gz - Go on and run the codes and compare the results to the previous fastqc and fastp outputs. - What do you think are the advantages and disadvantages of using any of the two trimming tools - fastp and porechop?.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nfastqc --threads 4 barcode30_porechopped.fastq.gz\nfastp --cut_front --cut_tail --trim_poly_x --cut_mean_quality 10 --qualified_quality_phred 10 --unqualified_percent_limit 10 --length_required 50 --in1 barcode30.fastq.gz --out1 barcode30_porechopped.fastq.gz --json barcode30_fastp.json --html barcode30_porechopped.fastp.html --thread 4"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#using-scp",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#using-scp",
    "title": "2.1 Sequencing QC",
    "section": "Using scp",
    "text": "Using scp\n\nTo copy a file from a remote server:\n\nscp ssh user@IP.address:/path/file_name /local/destination/path/\n\nTo copy a directory from a remote server:\n\nscp -r ssh user@IP.address:/path/directory[/] /local/destination/path/\n\nTo copy a file to a remote server:\n\nscp ssh /local/path/file_name user@IP.address:/destination/path/\n\nTo copy a directory to a remote server:\n\nscp -r ssh /local/path/directory[/]  user@IP.address:/destination/path/"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#using-rsync",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#using-rsync",
    "title": "2.1 Sequencing QC",
    "section": "Using rsync",
    "text": "Using rsync\nBecause Rsync transfers files recursively, you do not need to add the -r flag. You can use the following commands to transfer the files in an archived or compressed manner:\n\nTo copy a file from a remote server:\n\nrsync [-avz] user@IP.address:/path/file_name /local/destination/path/\n\nTo copy a directory from a remote server:\n\nrsync [-avz] user@IP.address:/path/directory[/] /local/destination/path/\n\nTo copy a file to a remote server:\n\nrsync [-avz] /local/path/file_name user@IP.address:/destination/path/\n\nTo copy a directory to a remote server:\n\nrsync [-avz] /local/path/directory[/]  user@IP.address:/destination/path/\nTo copy your html file from the server to your downloads, open a new terminal and cd to your downloads. Run the below command\nrsync -azhp student??@10.27.7.13:/home/student??/ONT/NanoPlot-report.html .\nscp -p ssh student??@10.27.7.13:/home/student??/ONT/NanoPlot-report.html ."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#identifying-contamination",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#identifying-contamination",
    "title": "2.1 Sequencing QC",
    "section": "2.1.5 Identifying contamination",
    "text": "2.1.5 Identifying contamination\nIt is always a good idea to check that your data is from the species you expect it to be. A very useful tool for this is Kraken. In this workshop we will go through how you can use Kraken to check your samples for contamination.\n\nKraken 2\nclick here to view the publication on Kraken 2 kraken is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.\nThe first version of Kraken used a large indexed and sorted list of -mer/LCA pairs as its database. While fast, the large memory requirements posed some problems for users, and so Kraken 2 was created to provide a solution to those problems.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for Kraken 2\nKraken2 --help\nUsage: kraken2 [options] <filename(s)>\n\nOptions:\n  --db NAME               Name for Kraken 2 DB\n                          (default: none)\n  --threads NUM           Number of threads (default: 1)\n  --quick                 Quick operation (use first hit or hits)\n  --unclassified-out FILENAME\n                          Print unclassified sequences to filename\n  --classified-out FILENAME\n                          Print classified sequences to filename\n  --output FILENAME       Print output to filename (default: stdout); \"-\" will\n                          suppress normal output\n  --confidence FLOAT      Confidence score threshold (default: 0.0); must be\n                          in [0, 1].\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nKraken 2 is run on a database. You don’t have to worry about the database for now. All has been setup for you. To set up the database yourself, visit the setup page\nThe general synthax for running Kraken 2 is:\nkraken2 --db $DBNAME [other_options] <filename(s)>\n\n\nTo run Kraken 2, you need to provide the path to the database. By default, the input files are assumed to be in FASTA format, so in this case we also need to tell Kraken that our input files are in FASTQ format, gzipped, and that they are ONT reads.\nYou can now go ahead and run Kraken 2 with the below command.\n\n\n\n\n\n\nNote\n\n\n\nTake note of these: 1. You have to specify the directory to the database 2. Our input files will be the porechopped fastq sequences from the fastp analysis.\nDo this to run Kraken 2: \nkraken2 --db ../database/minikraken2_v1_8GB --threads 8 --unclassified-out barcode30.unclassified#.fastq --classified-out barcode30.classified#.fastq --report barcode30.kraken2.report.txt --output barcode30.kraken2.out --gzip-compressed --report-zero-counts barcode30_porechopped.fastq.gz\nLoading database information... done.\n53949 sequences (251.01 Mbp) processed in 30.316s (106.8 Kseq/m, 496.79 Mbp/m).\n  51565 sequences classified (95.58%)\n  2384 sequences unclassified (4.42%)\n\n\nYou should now see some new files generated in your current directory. How many new files have been generated? Do you see that the fastq files are unzipped?\n\n\n\n\n\n\nLet’s save some disk space by zipping the two new fastq files created\n\n\n\ngzip *.fastq\n\n\nYou can try to read the two kraken report output text files created, but I’m sure that won’t make real sense to you.\n\n\nQuick look at Kraken 2 report\n\nRun the command to have a quick look at the Kraken 2 report.\nhead -n 20 barcode30.kraken2.report.txt\nThe six columns in this file are:\n\nPercentage of reads covered by the clade rooted at this taxon\nNumber of reads covered by the clade rooted at this taxon\nNumber of reads assigned directly to this taxon\nA rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. All other ranks are simply ‘-’.\nNCBI taxonomy ID\nScientific name\n\n\nJust like we did for the fastq-scan output, we will have to convert the Kraken 2 output to a more readable format. Before we do this, we will have to perform one more analysis on our sample. The Kraken 2 often does not provide exhaustive results. To re-estimate taxonomic abundance of the samples we have to do this with another tool called Bracken using the output of Kraken 2.\n\n\nBracken\nclick here to view the publication on Bracken Bracken is a companion program to Kraken 1, KrakenUniq, or Kraken 2 While Kraken classifies reads to multiple levels in the taxonomic tree, Bracken allows estimation of abundance at a single level using those classifications (e.g. Bracken can estimate abundance of species within a sample)\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for fastq-scan \nbracken -h\nUsage: bracken -d MY_DB -i INPUT -o OUTPUT -w OUTREPORT -r READ_LEN -l LEVEL -t THRESHOLD\n  MY_DB          location of Kraken database\n  INPUT          Kraken REPORT file to use for abundance estimation\n  OUTPUT         file name for Bracken default output\n  OUTREPORT      New Kraken REPORT output file with Bracken read estimates\n  READ_LEN       read length to get all classifications for (default: 100)\n  LEVEL          level to estimate abundance at [options: D,P,C,O,F,G,S,S1,etc] (default: S)\n  THRESHOLD      number of reads required PRIOR to abundance estimation to perform reestimation (default: 0)\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general synthax is:\nbracken -d ${KRAKEN_DB} -t ${THREADS} -k ${KMER_LEN} -l ${READ_LEN} -i {INPUT} -o {OUTPUT}\nNB. Bracken relies on the Kraken 2 database, so we will specify the same directory as before and also use the output report of Kraken 2 as our input.\n\n\nYou can now go ahead and perform Bracken with the below command.\n\n\n\n\n\n\nNote\n\n\n\nRun Bracken with the following command:\nbracken -l S -t 10 -d ../database/minikraken2_v1_8GB -i barcode30.kraken2.report.txt -o barcode30_bracken_S.tsv\n>> Checking for Valid Options...\n >> Running Bracken\n      >> python src/est_abundance.py -i barcode30.kraken2.report.txt -o barcode30_bracken_S.tsv -k ../database/minikraken2_v1_8GB/database100mers.kmer_distrib -l S -t 10\nPROGRAM START TIME: 08-09-2023 17:20:28\n>> Checking report file: barcode30.kraken2.report.txt\nBRACKEN SUMMARY (Kraken report: barcode30.kraken2.report.txt)\n    >>> Threshold: 10\n    >>> Number of species in sample: 11780\n      >> Number of species with reads > threshold: 10\n      >> Number of species with reads < threshold: 11770\n    >>> Total reads in sample: 53949\n      >> Total reads kept at species level (reads > threshold): 16065\n      >> Total reads discarded (species reads < threshold): 273\n      >> Reads distributed: 35185\n      >> Reads not distributed (eg. no species above threshold): 42\n      >> Unclassified reads: 2384\nBRACKEN OUTPUT PRODUCED: barcode30_bracken_S.tsv\nPROGRAM END TIME: 08-09-2023 17:20:28\n  Bracken complete.\n\n\n\nYou should now see two new files generated in your current directory. You can take a look at them as they are small files. You can already guess what specie we are working with from the barcode30_bracken_S.tsv file.\n\n\nProducing a more friendly species composition file\nImagine you have over 100 samples and have produced Bracken output files for each file using a simple bash script. Ho do you check each individual file to see what species abundance there is? This will be tedious right? How can we make things easier for us? For instance have them all in one .tsv file to examine at a go. Your guess is as good as mine. …Let’s write a script for that.\nTo do this, we can simply parse all the Kraken 2 and Bracken results into a more friendly .tsv file that summarises all the output into one file. We will achieve this with a simple python script.\n\nParse Kraken 2 and Bracken results files into tsv format\nWe will run a simple python script to achieve this purpose. In your working directory, you will find a file named kraken_parser.py. This is a simple python script that will do the job for us.\nWe can go ahead and execute that python script by running the command below:\npython kraken_parser.py\nYou should now see another file generated in your directory called Bracken_species_composition.tsv.\nNow let’s go ahead and have a look at the tsv file with the command below. Remember, we have run analysis on only one sample and so will expect only one line of results.\n\n\n\n\n\n\ncat Bracken_species_composition.tsv\nname    Escherichia coli    Salmonella enterica other\nbarcode30_bracken   59.43420856332985   34.6785021632105    5.887289273459658\n\n\n\n\nAlternatively, you can open the tsv file with any appropriate GUI software (excel or libreoffice)\nNow, what if we want to also see all the QC statistics in one go provided we performed the analysis for multiple samples (or in our case even for just one sample).\nThankfully, bioinformaticians are not sleeping, there is one final tool (MultiQC) we will discuss here which puts all that analysis we have performed into a single report that is viewable on a web browser, yes an .html file.\n\n\n\nMultiQC\nclick here to view the publication on MultiQC or check out their website\nMultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.\nReports are generated by scanning given directories for recognised log files. These are parsed and a single HTML report is generated summarising the statistics for all logs found. MultiQC reports can describe multiple analysis steps and large numbers of samples within a single plot, and multiple analysis tools making it ideal for routine fast quality control.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for multiqc\nmultiqc -h\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\nOnce installed, you can use MultiQC by navigating to your analysis directory (or a parent directory) and running the tool.\nThe general synthax is:\nmultiqc .\nYes, that’s it! MultiQC will scan the specified directory (. is the current dir) and produce a report detailing whatever it finds.\n\n\nYou can now go ahead and perform the MultiQC with the below command. We will only add the -f flag which overwrites any existing MultiQC reports.\n\n\n\n\n\n\nNote\n\n\n\nPerform the analysis with this command:\nmultiqc -f .\n\n\n\nThe report is created in multiqc_report.html by default. Tab-delimited data files are also created in a directory called multiqc_data/, containing extra information. These can be easily inspected using Excel.\nYou should now see two new files generated in your current directory.\nLet’s have a quick look the report by simply opening it in a web browser.\nWe should now know what each plot represent. If you don’t at this point, then you may want to start from the beginning of this document."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#qc-bash-script-putting-it-all-together",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#qc-bash-script-putting-it-all-together",
    "title": "2.1 Sequencing QC",
    "section": "2.1.6 QC bash script: Putting it all together",
    "text": "2.1.6 QC bash script: Putting it all together\nNow let’s try this out! We will generate QC stats for all five Nanopore reads. We will perform all the analysis above (except fastp) in one run while generating all the various files at each stage within the pipeline in tandem.\n\nRunning multiple samples with a simple bash script\nAll the fastq files needed for this analysis are located in the current working QC directory. You can have a look by simply running the below command to check out all the reads present in the current directory.\n\n\n\n\n\n\ncheck out all the reads present in the current directory\n\n\n\nls *.fastq.gz \nbarcode30.fastq.gz  barcode33.fastq.gz barcode34.fastq.gz barcode36.fastq.gz barcode40.fastq.gz\n\n\nLet’s have a look at the QC bash script we are going to run:\ncat qcloop_ont.sh\nThe script contains several commands, some are combined together using pipes. (UNIX pipes is a very powerful and elegant concept which allows us to feed the output of one command into the next command and avoid writing intermediate files. If you are not comfortable with UNIX or how scripts look like and how they work, consider having a go at the UNIX tutorial or more specifically bonus shell script.\nNow run the script to create the QC stats (this may take a while):\n\n\n\n\n\n\nbash qcloop.sh\n###########################################\n#### loop bash script for running QC ######\n###########################################\n#               #\n#  ___   ____   #\n# / _ \\ / ___|  #\n# | | | | |     #\n# | |_| | |___  #\n# \\__\\_\\ \\____| #\n#               #\n#               #\n############################################\nfastq-scan_parser.py exists and will run in a second.\n <<<<<<<running fastq-scan_parser>>>>>>>\nTraceback (most recent call last):\n...\n\n\n\nThe script will produce all the intermediate and final files we have encountered in this session.\nPerform a simple ls command with the arguments -lhrt and view the most recent files created.\nls -lhrt\nWhich files do you consider most useful?\nNow, let’s have a look at the two most important files here — The multiqc_report.html and the Bracken_species_composition.tsv.\n\n\n\n\n\n\nExercise 2.1.2: MultiQC and Species Composition Report Interpretation\n\n\n\nHave a quick look at both MultiQC report and Bracken species composition files and attempt these questions\n\nHow many genomes were analysed?\nWhat is the average read length of each read?\nWhat is the average GC content of each genome?\nWill you pass all the reads as being good reads? Given a phred quality score threshold of 20.\nIdentify the reads that failed QC and report what could be wrong with the sequences.\nWhat are the most abundant species identified in each of the genomes?\nAre there any suspected contaminants in any of the genomes?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nIf you are looking for a solution, I am sorry, no solution.\nIf you are having any challenges just hold on, we will solve this together in class.\n\n\n\n\n\nIf you have successfully gone through the exercise above, congratulations!!! You are a bioinformatics QC expert now.\nNow, let’s take particular note of the porechopped genomes that we are most confident in. We will carry out our de novo assembly of long reads from these genomes in our next lesson.\n\n\n\nDisk Usage II — Cleaning up after analysis\nNow that we are done investigating our genomic sequences, let’s pause again and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\nHow much disk space have you used since the start of the analysis? I’m sure it’s more than 4G. That’s fine. Let’s do some cleaning up. We will remove rm all files that we may not need for further analysis. Most of these files are intermediate files and can always be reproduced if we need to.\nYou will find this cleaning process very useful in the next chapter where we will generate tonnes and tonnes of data.\n\nremove all porechopped fastq files if not needed\nrm *porechopped*\n\n\nremove all classified and unclassified fastq files if not needed\nrm *classified*\n\n\nremove all Kraken 2 output if not needed\nrm *kraken2.out\n\n\n\n\n\n\n\nExercise 2.1.4: Advance Exercise\n\n\n\nIf you have gone through this material in good time and still want to try your hands on some more advanced stuff, you are free to do this exercise.\nTry modifying the qcloop_ont script to be able to carry out analysis on a specific pair of fastq reads given that you have more than one pair of fastq in your working directory (assuming you want to perform QC again on only one of the sequences that passed our quality checks).\nYou can call the new script `qcloop_ont_modified``\nYou should be able to call out the script to carry out the analysis on the specified fastq files with the below command\nbash qcloop_ont_modified barcode30.fastq.gz\n\n\n\n\n\n\nSolution:"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#deactivate-qc_ont-environment",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#deactivate-qc_ont-environment",
    "title": "2.1 Sequencing QC",
    "section": "2.1.7 deactivate qc_ont environment",
    "text": "2.1.7 deactivate qc_ont environment\nNow that we are done with all our analysis, let’s deactivate the qc_ont environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#credit",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.1-Sequencing_QC.html#credit",
    "title": "2.1 Sequencing QC",
    "section": "2.1.8 Credit",
    "text": "2.1.8 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/sanger-pathogens/QC-training\nhttps://github.com/wdecoster/NanoPlot\nhttps://github.com/rpetit3/fastq-scan\nhttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\nhttps://github.com/OpenGene/fastp\nhttps://github.com/rrwick/Porechop\nhttps://github.com/DerrickWood/kraken2\nhttps://github.com/jenniferlu717/Bracken\nhttps://github.com/ewels/MultiQC"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html",
    "title": "2.2 Assembly",
    "section": "",
    "text": "Teaching: 90 min || Exercises: 30 min"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#overview",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#overview",
    "title": "2.2 Assembly",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is de novo genome assembly?\nWhat is reference-based assembly?\nHow do I assemble my genome using ONT data?\nHow do I assess the quality of my assembled genome?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUnderstand what de novo genome assembly is and how it differs from reference-based assembly.\nAssemble sequence data with flye and polish with medaka\nGenerate metrics for your assembly with QUAST\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nBest practice assembly and polishing tools for Nanopore data include\n\nFlye\nUnicycler\nMiniasm\nRacon\nCanu\nMedaka\nNanoPolish\nQUAST\n\nde novo genome assembly involves assembling a genome from the raw reads without the aid of a reference genome as against a reference-based assembly which relies on a reference genome for alignment and subsequent assembly"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#background",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#background",
    "title": "2.2 Assembly",
    "section": "2.2.1 Background",
    "text": "2.2.1 Background\nThere are two approaches for genome assembly: reference-based (or comparative) or de novo. In a reference-based assembly, we use a reference genome as a guide to map our sequence data to and thus reassemble our sequence this way (we will not do this here with our ONT data but will try it out with illumina data later on in the course). Alternatively, we can create a ‘new’ (de novo) assembly that does not rely on a map or reference and more closely reflects the actual genome structure of the isolate that was sequenced.\n\n\n\nGenome assembly\n\n\n\nGenome assemblers\nSeveral tools are available for de novo genome assembly depending on whether you’re trying to assemble short-read sequence data, long reads or else a combination of both. Three of the most commonly used assemblers for long read data are Flye, Unicycler and Miniasm.\nNow that we have been able to see how Quality our Nanopore sequences are, we can can confidently proceed to assemble our bacterial genome. Here, we will perform long read assembly utilizing Flye in combination with Medaka. Other useful tools for assembly includes Unicycler and Miniasm in combination with Racon, or Canu. Another polishing tool in addition to Medaka is NanoPolish with Fast5 files. Each of these tools, have their pros and cons but we will not go deep into them here. Following the assembly, we will QC the products using QUAST.\n\n\nDisk Usage I — Before analysis\n\nBefore we start performing any assemblies, let’s pause and check the space of our current working directory as we did for our previous lesson.\nYou can do this with the disk usage du command\ndu -h\n\n\nCurrent Disk Space In assembly_annotation_MTB Directory\n\n~247MB\n\nNow, keep this value in mind, and this time, don’t forget it. We will come back to it at the end of this chapter."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#navigate-to-the-ont-directory-and-activate-the-appropriate-environment-flye-medaka-quast-as-we-go-along",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#navigate-to-the-ont-directory-and-activate-the-appropriate-environment-flye-medaka-quast-as-we-go-along",
    "title": "2.2 Assembly",
    "section": "2.2.2 Navigate to the ONT directory and activate the appropriate environment [flye, medaka, quast] as we go along",
    "text": "2.2.2 Navigate to the ONT directory and activate the appropriate environment [flye, medaka, quast] as we go along\nNote that, due to package conflicts, we are not currently able to place all of the above tools in the same conda environment. Before we start, navigate into the ONT/ directory and activate the flye environment:\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/ONT/\nmamba activate flye"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#assembly-and-genome-polishing",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#assembly-and-genome-polishing",
    "title": "2.2 Assembly",
    "section": "2.2.3 Assembly and Genome Polishing",
    "text": "2.2.3 Assembly and Genome Polishing\n\nde novo genome assembly with Flye\nFlye is a de novo assembler for single-molecule sequencing reads, such as those produced by PacBio and Oxford Nanopore Technologies. It is designed for a wide range of datasets, from small bacterial projects to large mammalian-scale assemblies. The package represents a complete pipeline: it takes raw PacBio / ONT reads as input and outputs polished contigs. Flye also has a special mode for metagenome assembly.\nNanopore assemblers are still in development, but Flye is a de facto standard.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for flye\nflye -h\nusage: flye (--pacbio-raw | --pacbio-corr | --pacbio-hifi | --nano-raw |\n         --nano-corr | --nano-hq ) file1 [file_2 ...]\n         --out-dir PATH\n\n         [--genome-size SIZE] [--threads int] [--iterations int]\n         [--meta] [--polish-target] [--min-overlap SIZE]\n         [--keep-haplotypes] [--debug] [--version] [--help]\n         [--scaffold] [--resume] [--resume-from] [--stop-after]\n         [--read-error float] [--extra-params]\n         [--deterministic]\n\nAssembly of long reads with repeat graphs\n\noptions:\n  -h, --help            show this help message and exit\n  --pacbio-raw path [path ...]\n                        PacBio regular CLR reads (<20% error)\n\n...\n\n\n\nUsage\n\n\n\n\n\n\nThe general format of the command is:\nflye  --nano-raw file1 [file_2 ...] --out-dir PATH\n\n\n\nWe will attempt to start flye by running the below command on our first dataset barcode30. Note that, here we will use our trimmed dataset barcode30_porechopped.fastq.gz.\n\n\n\n\n\n\nflye  --nano-raw barcode30_porechopped.fastq.gz --out-dir barcode30_flye\n[2023-08-10 09:28:51] INFO: Starting Flye 2.9.2-b1786\n[2023-08-10 09:28:51] INFO: >>>STAGE: configure\n[2023-08-10 09:28:51] INFO: Configuring run\n[2023-08-10 09:28:55] INFO: Total read length: 251007904\n[2023-08-10 09:28:55] INFO: Reads N50/N90: 8665 / 2409\n[2023-08-10 09:28:55] INFO: Minimum overlap set to 2000\n[2023-08-10 09:28:55] INFO: >>>STAGE: assembly\n[2023-08-10 09:28:55] INFO: Assembling disjointigs\n[2023-08-10 09:28:56] INFO: Reading sequences\n[2023-08-10 09:29:03] INFO: Counting k-mers:\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\n[2023-08-10 09:30:52] INFO: Filling index table (1/2)\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\n[2023-08-10 09:32:37] INFO: Filling index table (2/2)\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\n[2023-08-10 09:35:43] INFO: Extending reads\n[2023-08-10 09:37:04] INFO: Overlap-based coverage: 38\n[2023-08-10 09:37:04] INFO: Median overlap divergence: 0.073935\n...\n...\n[2023-08-10 09:53:46] INFO: Alignment error rate: 0.074446\n[2023-08-10 09:53:46] INFO: Correcting bubbles\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\n[2023-08-10 09:59:12] INFO: >>>STAGE: finalize\n[2023-08-10 09:59:12] INFO: Assembly statistics:\n\n    Total length:   5551201\n    Fragments:  14\n    Fragments N50:  3514571\n    Largest frg:    3514571\n    Scaffolds:  0\n    Mean coverage:  44\n\nNB. If the input reads were corrected, use --nano-corr instead of --nano-raw.\nThe following files will be created in the barcode30_flye directory:\n\n\n\nRelevant output files\nDescription\n\n\n\n\nassembly.fasta\nThe final assembly you should use\n\n\nassembly_info.txt\nContig summary information\n\n\nassembly_graph.gfa\nAssembly graph\n\n\nflye.log\nFull log file for bug reporting\n\n\n\n\n\n\n\nLet’s have quick look at the assembly_info.txt file and attempt to explain it’s contents.\n\n\n\n\n\n\nPeep into assembly_info.txt\n\n\n\ncat barcode30_flye/assembly_info.txt\n#seq_name   length  cov.    circ.   repeat  mult.   alt_group   graph_path\ncontig_2    3514571 41  N   N   1   *   -3,2,-3\ncontig_1    1663577 45  N   N   1   *   3,1,3\ncontig_10   141945  29  Y   N   1   *   10\ncontig_16   81727   39  Y   N   1   *   16\ncontig_5    46307   54  N   Y   1   6   *,5,*\ncontig_3    42686   42  N   Y   1   *   3\ncontig_4    17614   12  N   Y   1   6   *,4,*\ncontig_18   9539    797 Y   Y   19  *   18\ncontig_14   8359    18  N   Y   3   8   *,14,*\ncontig_9    8110    1108    N   Y   26  *   9\ncontig_12   5813    22  N   Y   1   *   12\ncontig_17   5810    41  N   Y   1   *   17\ncontig_6    4632    10  N   N   10  *   *,6,*\ncontig_8    511 49  Y   Y   1   2   8\n\n\n\nWe can obtain most of the above information by just applying very simple commandline tools to interogate the assembly.fasta file.\n\n\n\n\n\n\nExercise 2.2.1: Obtain statistics from assembly.fasta file.\n\n\n\n\nWhat simple command can you use to have a peep into the assembly.fasta file by looking at only the first 10 lines.\nWhat quick command can you use to count all the lines and letters in the assembly.fasta file.\nWith one line of command, how would you determine the number of contigs present in the assembly.fasta file.\n\nReport on your findings. How do they varry from the information contained in the assembly_info.txt file.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nTo view the first 10 lines:\n\nhead -n 10 assembly.fasta\nTo view the last 10 lines\ntail -n 10 assembly.fasta\n\n\n\nwc assembly.fasta\n\n\n\ngrep -c '>' assembly.fasta\n\n\n\n\n\nBefore we move on to the next step, let’s rename our assembly (assembly.fasta) to something more meaningful barcode30_assembly.fasta and copy it to the current directory.\nHow do we do this?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ncp barcode30_flye/assembly.fasta barcode30_assembly.fasta\n\n\n\n\n\n\ngenome polishing with Medaka\nNow that we are done with flye, let’s deactivate the flye environment and activate our medaka environment.\nmamba deactivate\nmamba activate medaka\nMedaka is an assembly polisher and variant caller made by ONT and used to create consensus sequences and variant calls from nanopore sequencing data. It is the recommended polisher for Flye assemblies. The Medaka documentation mentions that it has specifically been trained on Flye output (it is an ML-based tool).\nMedaka requires only basecalled data — .fasta or .fastq and it is 50X faster than Nanopolish.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for Medaka\nmedaka -h\nusage: medaka [-h] [--version]\n              {compress_bam,features,train,consensus,smolecule,consensus_from_features,fastrle,stitch,variant,snp,tools}\n              ...\n\noptions:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n\nsubcommands:\n  valid commands\n\n  {compress_bam,features,train,consensus,smolecule,consensus_from_features,fastrle,stitch,variant,snp,tools}\n                        additional help\n    compress_bam        Compress an alignment into RLE form.\n    features            Create features for inference.\n    train               Train a model from features.\n    consensus           Run inference from a trained model and alignments.\n    smolecule           Create consensus sequences from single-molecule reads.\n    consensus_from_features\n                        Run inference from a trained model on existing features.\n    fastrle             Create run-length encoded fastq (lengths in quality track).\n    stitch              Stitch together output from medaka consensus into final output.\n    variant             Decode probabilities to VCF.\n    snp                 Decode probabilities to SNPs.\n    tools               tools subcommand.\n\n\n\nUsage\n\n\n\n\n\n\nThe general format of the command is:\n\n\n\nmedaka_consensus -i ${BASECALLS} -d ${DRAFT} -o ${OUTDIR} -t ${NPROC} -m r941_min_high_g303 \n\n\nWe will polish our flye assembly by running the following command:\n\n\n\n\n\n\nPolish flye output\n\n\n\nmedaka_consensus -i barcode30_porechopped.fastq.gz -d barcode30_assembly.fasta -o barcode30_medaka -t 8 -m r10_min_high_g340\nChecking program versions\nThis is medaka 1.8.0\nProgram    Version    Required   Pass     \nbcftools   1.17       1.11       True     \nbgzip      1.17       1.11       True     \nminimap2   2.26       2.11       True     \nsamtools   1.17       1.11       True     \ntabix      1.17       1.11       True     \nAligning basecalls to draft\n...\n...\n[12:55:23 - PWorker] Processed 3 batches\n[12:55:23 - PWorker] All done, 0 remainder regions.\n[12:55:23 - Predict] Finished processing all regions.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - TrimOlap] disjointig_2:22835.0-30849.0 and disjointig_2:85459.0-91200.0 cannot be concatenated as there is no overlap and they do not abut.\n[12:55:25 - DataIndx] Loaded 1/1 (100.00%) sample files.\n[12:55:25 - TrimOlap] disjointig_3:63937.0-73640.0 and disjointig_3:78942.0-88372.0 cannot be concatenated as there is no overlap and they do not abut.\n[12:55:25 - TrimOlap] disjointig_1:4078692.0-4088250.0 and disjointig_1:4088272.0-4098065.0 cannot be concatenated as there is no overlap and they do not abut.\n[12:55:25 - TrimOlap] disjointig_1:4124178.0-4133945.0 and disjointig_1:4133977.0-4143803.0 cannot be concatenated as there is no overlap and they do not abut.\nPolished assembly written to barcode30_medaka/consensus.fasta, have a nice day.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf Medaka runs out of memory, add option -b 80 to the command-line. If it still runs out of memory, reduce the 80 further until it doesn’t.\n\n\nJust like we did for the assembly.fasta file generated from flye, let’s rename the consensus.fasta file to a more meaningful name.\n\n\n\n\n\n\nRename consensus file and copy to current directory\n\n\n\ncp barcode30_medaka/consensus.fasta barcode30_polished_assembly.fasta\n\n\n\n\n\n\n\n\nSetting the righ model -m.\n\n\n\nFor best results it is important to specify the correct model, -m in the above command we just run, according to the basecaller used. Allowed values can be found by running medaka tools list\\_models.\nMedaka models are named to indicate\n\nthe pore type,\nthe sequencing device (MinION or PromethION),\nthe basecaller variant,\nthe basecaller version, with the format:\n\n{pore}_{device}_{caller variant}_{caller version}\nFor example the model named r941_min_fast_g303 should be used with data from MinION (or GridION) R9.4.1 flowcells using the fast Guppy basecaller version 3.0.3. By contrast the model r941_prom_hac_g303 should be used with PromethION data and the high accuracy basecaller (termed “hac” in Guppy configuration files). Where a version of Guppy has been used without an exactly corresponding medaka model, the medaka model with the highest version equal to or less than the guppy version should be selected.\n\n\n\n\n\n\n\n\nBacterial (ploidy-1) variant calling\n\n\n\nNote that we can perform variant calling with medaka. Variant calling for monoploid samples is enabled through the medaka_haploid_variant workflow:\nmedaka_haploid_variant -i <reads.fastq> -r <ref.fasta>\nThis requires the reads as a .fasta or .fastq and a reference sequence as a .fasta file."
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#assembly-quality-assessment-with-quast",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#assembly-quality-assessment-with-quast",
    "title": "2.2 Assembly",
    "section": "2.2.4 Assembly quality assessment with QUAST",
    "text": "2.2.4 Assembly quality assessment with QUAST\nNow that we are done with medaka, let’s deactivate the medaka environment and activate our quast environment.\nmamba deactivate\nmamba activate quast\n\n\n\nGenome assembly evaluation tool\n\n\nBefore we do any further analyses with our assemblies, we need to assess the quality. To do this we use a tool called QUAST. QUAST stands for QUality ASsessment Tool. It evaluates genome/metagenome assemblies by computing and generating various metrics for assemblies.\nThese include but aren’t limited to:\n\nThe total number of contigs greater than 0 bp. Ideally, we like to see assemblies in the smallest number of contigs (‘pieces’) as this means there is likely less missing data contained in gaps between contigs.\nThe total length of the assembly. We normally know what the expected length of our assembly is (for more diverse organisms like E. coli there may be a range of genome sizes). The length of your assembly should be close to the expected genome size. If it’s too big or too small, either there is some kind of contamination or else you’ve sequenced the wrong species!\nThe N50 of the assembly. This is the final metric often used to assess the quality of an assembly. The N50 is typically calculated by averaging the length of the largest contigs that account for 50% of the genome size. This is a bit more complicated conceptually but the higher the N50 the better. A large N50 implies that you have a small number of larger contigs in your assembly which equals a good assembly.\n\nWe will run quast in the same assembly environment.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for QUAST:\nquast -h\nQUAST: Quality Assessment Tool for Genome Assemblies\nVersion: 5.2.0\n\nUsage: python /home/ajv37/.conda/envs/quast/bin/quast [options] <files_with_contigs>\n\nOptions:\n-o  --output-dir  <dirname>       Directory to store all result files [default: quast_results/results_<datetime>]\n-r                <filename>      Reference genome file\n-g  --features [type:]<filename>  File with genomic feature coordinates in the reference (GFF, BED, NCBI or TXT)\n                                  Optional 'type' can be specified for extracting only a specific feature type from GFF\n-m  --min-contig  <int>           Lower threshold for contig length [default: 500]\n-t  --threads     <int>           Maximum number of threads [default: 25% of CPUs]\n\nAdvanced options:\n-s  --split-scaffolds                 Split assemblies by continuous fragments of N's and add such \"contigs\" to the comparison\n-l  --labels \"label, label, ...\"      Names of assemblies to use in reports, comma-separated. If contain spaces, use quotes\n-L                                    Take assembly names from their parent directory names\n-e  --eukaryote                       Genome is eukaryotic (primarily affects gene prediction)\n    --fungus                          Genome is fungal (primarily affects gene prediction)\n    --large                           Use optimal parameters for evaluation of large genomes\n                                      In particular, imposes '-e -m 3000 -i 500 -x 7000' (can be overridden manually)\n-k  --k-mer-stats                     Compute k-mer-based quality metrics (recommended for large genomes)\n                                      This may significantly increase memory and time consumption on large genomes\n    --k-mer-size                      Size of k used in --k-mer-stats [default: 101]\n    --circos                          Draw Circos plot\n-f  --gene-finding                    Predict genes using GeneMarkS (prokaryotes, default) or GeneMark-ES (eukaryotes, use --eukaryote)\n    --mgm                             Use MetaGeneMark for gene prediction (instead of the default finder, see above)\n    --glimmer                         Use GlimmerHMM for gene prediction (instead of the default finder, see above)\n    --gene-thresholds <int,int,...>   Comma-separated list of threshold lengths of genes to search with Gene Finding module\n                                      [default: 0,300,1500,3000]\n    --rna-finding                     Predict ribosomal RNA genes using Barrnap\n-b  --conserved-genes-finding         Count conserved orthologs using BUSCO (only on Linux)\n    --operons  <filename>             File with operon coordinates in the reference (GFF, BED, NCBI or TXT)\n    --est-ref-size <int>              Estimated reference size (for computing NGx metrics without a reference)\n    --contig-thresholds <int,int,...> Comma-separated list of contig length thresholds [default: 0,1000,5000,10000,25000,50000]\n-u  --use-all-alignments              Compute genome fraction, # genes, # operons in QUAST v1.* style.\n                                      By default, QUAST filters Minimap's alignments to keep only best ones\n-i  --min-alignment <int>             The minimum alignment length [default: 65]\n    --min-identity <float>            The minimum alignment identity (80.0, 100.0) [default: 95.0]\n-a  --ambiguity-usage <none|one|all>  Use none, one, or all alignments of a contig when all of them\n                                      are almost equally good (see --ambiguity-score) [default: one]\n    --ambiguity-score <float>         Score S for defining equally good alignments of a single contig. All alignments are sorted \n                                      by decreasing LEN * IDY% value. All alignments with LEN * IDY% < S * best(LEN * IDY%) are \n                                      discarded. S should be between 0.8 and 1.0 [default: 0.99]\n    --strict-NA                       Break contigs in any misassembly event when compute NAx and NGAx.\n                                      By default, QUAST breaks contigs only by extensive misassemblies (not local ones)\n-x  --extensive-mis-size  <int>       Lower threshold for extensive misassembly size. All relocations with inconsistency\n                                      less than extensive-mis-size are counted as local misassemblies [default: 1000]\n    --scaffold-gap-max-size  <int>    Max allowed scaffold gap length difference. All relocations with inconsistency\n                                      less than scaffold-gap-size are counted as scaffold gap misassemblies [default: 10000]\n    --unaligned-part-size  <int>      Lower threshold for detecting partially unaligned contigs. Such contig should have\n                                      at least one unaligned fragment >= the threshold [default: 500]\n    --skip-unaligned-mis-contigs      Do not distinguish contigs with >= 50% unaligned bases as a separate group\n                                      By default, QUAST does not count misassemblies in them\n    --fragmented                      Reference genome may be fragmented into small pieces (e.g. scaffolded reference) \n    --fragmented-max-indent  <int>    Mark translocation as fake if both alignments are located no further than N bases \n                                      from the ends of the reference fragments [default: 85]\n                                      Requires --fragmented option\n    --upper-bound-assembly            Simulate upper bound assembly based on the reference genome and reads\n    --upper-bound-min-con  <int>      Minimal number of 'connecting reads' needed for joining upper bound contigs into a scaffold\n                                      [default: 2 for mate-pairs and 1 for long reads]\n    --est-insert-size  <int>          Use provided insert size in upper bound assembly simulation [default: auto detect from reads or 255]\n    --plots-format  <str>             Save plots in specified format [default: pdf].\n                                      Supported formats: emf, eps, pdf, png, ps, raw, rgba, svg, svgz\n    --memory-efficient                Run everything using one thread, separately per each assembly.\n                                      This may significantly reduce memory consumption on large genomes\n    --space-efficient                 Create only reports and plots files. Aux files including .stdout, .stderr, .coords will not be created.\n                                      This may significantly reduce space consumption on large genomes. Icarus viewers also will not be built\n-1  --pe1     <filename>              File with forward paired-end reads (in FASTQ format, may be gzipped)\n-2  --pe2     <filename>              File with reverse paired-end reads (in FASTQ format, may be gzipped)\n    --pe12    <filename>              File with interlaced forward and reverse paired-end reads. (in FASTQ format, may be gzipped)\n    --mp1     <filename>              File with forward mate-pair reads (in FASTQ format, may be gzipped)\n    --mp2     <filename>              File with reverse mate-pair reads (in FASTQ format, may be gzipped)\n    --mp12    <filename>              File with interlaced forward and reverse mate-pair reads (in FASTQ format, may be gzipped)\n    --single  <filename>              File with unpaired short reads (in FASTQ format, may be gzipped)\n    --pacbio     <filename>           File with PacBio reads (in FASTQ format, may be gzipped)\n    --nanopore   <filename>           File with Oxford Nanopore reads (in FASTQ format, may be gzipped)\n    --ref-sam <filename>              SAM alignment file obtained by aligning reads to reference genome file\n    --ref-bam <filename>              BAM alignment file obtained by aligning reads to reference genome file\n    --sam     <filename,filename,...> Comma-separated list of SAM alignment files obtained by aligning reads to assemblies\n                                      (use the same order as for files with contigs)\n    --bam     <filename,filename,...> Comma-separated list of BAM alignment files obtained by aligning reads to assemblies\n                                      (use the same order as for files with contigs)\n                                      Reads (or SAM/BAM file) are used for structural variation detection and\n                                      coverage histogram building in Icarus\n    --sv-bedpe  <filename>            File with structural variations (in BEDPE format)\n\nSpeedup options:\n    --no-check                        Do not check and correct input fasta files. Use at your own risk (see manual)\n    --no-plots                        Do not draw plots\n    --no-html                         Do not build html reports and Icarus viewers\n    --no-icarus                       Do not build Icarus viewers\n    --no-snps                         Do not report SNPs (may significantly reduce memory consumption on large genomes)\n    --no-gc                           Do not compute GC% and GC-distribution\n    --no-sv                           Do not run structural variation detection (make sense only if reads are specified)\n    --no-gzip                         Do not compress large output files\n    --no-read-stats                   Do not align reads to assemblies\n                                      Reads will be aligned to reference and used for coverage analysis,\n                                      upper bound assembly simulation, and structural variation detection.\n                                      Use this option if you do not need read statistics for assemblies.\n    --fast                            A combination of all speedup options except --no-check\n\nOther:\n    --silent                          Do not print detailed information about each step to stdout (log file is not affected)\n    --test                            Run QUAST on the data from the test_data folder, output to quast_test_output\n    --test-sv                         Run QUAST with structural variants detection on the data from the test_data folder,\n                                      output to quast_test_output\n-h  --help                            Print full usage message\n-v  --version                         Print version\n\nOnline QUAST manual is available at http://quast.sf.net/manual\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe basic command for running QUAST is as follows:\nquast.py --output-dir <OUTDIR> <ASSEMBLY>  \nThe meaning of the option used is:\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\n–output-dir\nDIRECTORY\nDirectory to write the output files to\n\n\n\n\n\n\n\n\n\n\n\nRun QUAST\n\n\n\nLet’s run QUAST on the polished genome we’ve just created to assess the quality:\nquast.py --output-dir quast barcode30_polished_assembly.fasta\nVersion: 5.2.0\n\nSystem information:\n  OS: Linux-5.15.0-78-generic-x86_64-with-glibc2.35 (linux_64)\n  Python version: 3.10.8\n  CPUs number: 4\n\nStarted: 2023-08-10 15:22:18\n\nLogging to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/quast.log\nNOTICE: Maximum number of threads is set to 1 (use --threads option to set it manually)\n\nCWD: /home/prince/Dropbox/Ubuntu/acdc/trial\nMain parameters:\n  MODE: default, threads: 1, min contig length: 500, min alignment length: 65, min alignment IDY: 95.0, \\\n  ambiguity: one, min local misassembly length: 200, min extensive misassembly length: 1000\n\nContigs:\n  Pre-processing...\n  barcode30_polished_assembly.fasta ==> barcode30_polished_assembly\n\n2023-08-10 15:22:22\nRunning Basic statistics processor...\n  Contig files:\n    barcode30_polished_assembly\n  Calculating N50 and L50...\n    barcode30_polished_assembly, N50 = 3514823, L50 = 1, auN = 2730259.6, Total length = 5550020, GC % = 50.53, # N's per 100 kbp =  0.00\n  Drawing Nx plot...\n    saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/basic_stats/Nx_plot.pdf\n  Drawing cumulative plot...\n    saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/basic_stats/cumulative_plot.pdf\n  Drawing GC content plot...\n    saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/basic_stats/GC_content_plot.pdf\n  Drawing barcode30_polished_assembly GC content plot...\n    saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/basic_stats/barcode30_polished_assembly_GC_content_plot.pdf\nDone.\n\nNOTICE: Genes are not predicted by default. Use --gene-finding or --glimmer option to enable it.\n\n2023-08-10 15:22:24\nCreating large visual summaries...\nThis may take a while: press Ctrl-C to skip this step..\n  1 of 2: Creating PDF with all tables and plots...\n  2 of 2: Creating Icarus viewers...\nDone\n\n2023-08-10 15:22:25\nRESULTS:\n  Text versions of total report are saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/report.txt, report.tsv, and report.tex\n  Text versions of transposed total report are saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/transposed_report.txt, transposed_report.tsv, and transposed_report.tex\n  HTML version (interactive tables and plots) is saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/report.html\n  PDF version (tables and plots) is saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/report.pdf\n  Icarus (contig browser) is saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/icarus.html\n  Log is saved to /home/prince/Dropbox/Ubuntu/acdc/trial/quast/quast.log\n\nFinished: 2023-08-10 15:22:25\nElapsed time: 0:00:07.248144\nNOTICEs: 2; WARNINGs: 0; non-fatal ERRORs: 0\n\nThank you for using QUAST!\n\nThe following files will be created in the quast directory:\n\n\n\n\n\n\n\nOutput file\nDescription\n\n\n\n\nreport.txt\nAssessment summary in plain text format\n\n\nreport.tsv\nTab-separated version of the summary, suitable for spreadsheets (Google Docs, Excel, etc)\n\n\nreport.tex\nLaTeX version of the summary\n\n\nicarus.html\nIcarus main menu with links to interactive viewers\n\n\nreport.pdf\nAll other plots combined with all tables (file is created if matplotlib python library is installed)\n\n\nreport.html\nHTML version of the report with interactive plots inside\n\n\ntransposed_report.txt\nTransposed assessment summary in plain text format\n\n\ntransposed_report.tsv\nTransposed tab-separated version of the summary\n\n\ntransposed_report.tex\nTransposed LaTeX version of the summary\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2.2: Investigate the QUAST output\n\n\n\nNow that we’ve got the results from QUAST, how good is the assembly we created with flye and polished with medaka?\n\nWhat is the total number of contigs?\nWhat is the total length of the assembly?\nWhat is the N50 of the assembly?\nWhat is the %GC content?\nHave a look at the report.html and report.pdffiles\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThere are a couple of ways we could answer the questions above as the assembly metrics can be parsed in different ways. Let’s parse the report.tsv:\ncat quast/report.tsv\nThis will provide all the information we need:\nAssembly    barcode30_polished_assembly\n# contigs (>= 0 bp) 14\n# contigs (>= 1000 bp)  13\n# contigs (>= 5000 bp)  12\n# contigs (>= 10000 bp) 7\n# contigs (>= 25000 bp) 6\n# contigs (>= 50000 bp) 4\nTotal length (>= 0 bp)  5550020\nTotal length (>= 1000 bp)   5549509\nTotal length (>= 5000 bp)   5544902\nTotal length (>= 10000 bp)  5507314\nTotal length (>= 25000 bp)  5489744\nTotal length (>= 50000 bp)  5400737\n# contigs   14\nLargest contig  3514823\nTotal length    5550020\nGC (%)  50.53\nN50 3514823\nN90 1663737\nauN 2730259.6\nL50 1\nL90 2\n# N's per 100 kbp   0.00\n\n\nThe total number of contigs > 0bp is 14\nThe total length of the assembly is 5550020\nThe N50 of the assembly is 3514823\nThe %GC content is 50.53%\n\nWhat do you think? Should we move forward with this assembly?\n\n\n\n\n\nFinally, deactivate the quast environment:\nmamba deactivate\n\n\n\n\n\n\nExercise 2.2.3: Running a bash script on all five samples and providing a MultiQC summary\n\n\n\nHave a quick look at the bash script assembly_ont.sh. Run the script and report on your findings.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nWe will run the bash script with the command below:\nbash assembly_ont.sh\n\n\n\n\n\n\n\nDisk Usage II — Cleaning up after analysis\n\nNow that we are done investigating our assembling and annotating our genome, let’s pause again and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\nHow much disk space have you used since the start of the analysis?"
  },
  {
    "objectID": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#credit",
    "href": "materials/02-ONT_Sequencing_QC_&_Assembly/2.2-Assembly.html#credit",
    "title": "2.2 Assembly",
    "section": "Credit",
    "text": "Credit\nSome information on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/Joseph7e/MDIBL-T3-WGS-Tutorial#genome-assembly\nhttps://github.com/fenderglass/Flye\nhttps://github.com/nanoporetech/medaka\nhttps://quast.sourceforge.net/quast"
  },
  {
    "objectID": "materials/03-Main_Topic_2/3.2-Topic_3.2.html",
    "href": "materials/03-Main_Topic_2/3.2-Topic_3.2.html",
    "title": "3.2 Topic 3.2",
    "section": "",
    "text": "This page is under construction"
  },
  {
    "objectID": "materials/03-Main_Topic_2/3.1-Topic_3.1.html",
    "href": "materials/03-Main_Topic_2/3.1-Topic_3.1.html",
    "title": "3.1 Topic 3.1",
    "section": "",
    "text": "This page is under construction"
  },
  {
    "objectID": "materials/template_file.html",
    "href": "materials/template_file.html",
    "title": "Title of Page",
    "section": "",
    "text": "materials in markdown"
  },
  {
    "objectID": "unix_cheat_sheet.html",
    "href": "unix_cheat_sheet.html",
    "title": "Unix Cheat Sheet",
    "section": "",
    "text": "This document gives a brief summary of useful Unix commands."
  },
  {
    "objectID": "unix_cheat_sheet.html#documentation-and-help",
    "href": "unix_cheat_sheet.html#documentation-and-help",
    "title": "Unix Cheat Sheet",
    "section": "Documentation and Help",
    "text": "Documentation and Help\n\n\n\n\n\n\n\nman {command}\nmanual page for the program\n\n\nwhatis {command}\nshort description of the program\n\n\n{command} --help\nmany programs use the --help flag to print documentation"
  },
  {
    "objectID": "unix_cheat_sheet.html#listing-files",
    "href": "unix_cheat_sheet.html#listing-files",
    "title": "Unix Cheat Sheet",
    "section": "Listing files",
    "text": "Listing files\n\n\n\n\n\n\n\nls\nlist files in the current directory\n\n\nls {path}\nlist files in the specified path\n\n\nls -l {path}\nlist files in long format (more information)\n\n\nls -a {path}\nlist all files (including hidden files)"
  },
  {
    "objectID": "unix_cheat_sheet.html#change-directories",
    "href": "unix_cheat_sheet.html#change-directories",
    "title": "Unix Cheat Sheet",
    "section": "Change Directories",
    "text": "Change Directories\n\n\n\n\n\n\n\ncd {path}\nchange to the specified directory\n\n\ncd or cd ~\nchange to the home directory\n\n\ncd ..\nmove back one directory\n\n\npwd\nprint working directory. Shows the full path of where you are at the moment (useful if you are lost)"
  },
  {
    "objectID": "unix_cheat_sheet.html#make-or-remove-directories",
    "href": "unix_cheat_sheet.html#make-or-remove-directories",
    "title": "Unix Cheat Sheet",
    "section": "Make or Remove Directories",
    "text": "Make or Remove Directories\n\n\n\n\n\n\n\nmkdir {dirname}\ncreate a directory with specified name\n\n\nrmdir {dirname}\nremove a directory (only works if the directory is empty)\n\n\nrm -r {dirname}\nremove the directory and all it’s contents (use with care)"
  },
  {
    "objectID": "unix_cheat_sheet.html#copy-move-and-remove-files",
    "href": "unix_cheat_sheet.html#copy-move-and-remove-files",
    "title": "Unix Cheat Sheet",
    "section": "Copy, Move and Remove Files",
    "text": "Copy, Move and Remove Files\n\n\n\n\n\n\n\ncp {source/path/file1} {target/path/}\ncopy “file1” to another directory keeping its name\n\n\ncp {source/path/file1} {target/path/file2}\ncopy “file1” to another directory naming it “file2”\n\n\ncp {file1} {file2}\nmake a copy of “file1” in the same directory with a new name “file2”\n\n\nmv {source/path/file1} {target/path/}\nmove “file1” to another directory keeping its name\n\n\nmv {source/path/file1} {target/path/file2}\nmove “file1” to another directory renaming it as “file2”\n\n\nmv {file1} {file2}\nis equivalent to renaming a file\n\n\nrm {filename}\nremove a file"
  },
  {
    "objectID": "unix_cheat_sheet.html#view-text-files",
    "href": "unix_cheat_sheet.html#view-text-files",
    "title": "Unix Cheat Sheet",
    "section": "View Text Files",
    "text": "View Text Files\n\n\n\n\n\n\n\nless {file}\nview and scroll through a text file\n\n\nhead {file}\nprint the first 10 lines of a file\n\n\nhead -n {N} {file}\nprint the first N lines of a file\n\n\ntail {file}\nprint the last 10 lines of a file\n\n\ntail -n {N} {file}\nprint the last N lines of a file\n\n\nhead -n {N} {file} | tail -n 1\nprint the Nth line of a file\n\n\ncat {file}\nprint the whole content of the file\n\n\ncat {file1} {file2} {...} {fileN}\nconcatenate files and print the result\n\n\nzcat {file} and zless {file}\nlike cat and less but for compressed files (.zip or .gz)"
  },
  {
    "objectID": "unix_cheat_sheet.html#find-patterns",
    "href": "unix_cheat_sheet.html#find-patterns",
    "title": "Unix Cheat Sheet",
    "section": "Find Patterns",
    "text": "Find Patterns\nFinding (and replacing) patterns in text is a very powerful feature of several command line programs. The patterns are specified using regular expressions (shortened as regex), which are not covered in this document. See this Regular Expressions Cheat Sheet for a comprehensive overview.\n\n\n\n\n\n\n\ngrep {regex} {file}\nprint the lines of the file that have a match with the regular expression pattern"
  },
  {
    "objectID": "unix_cheat_sheet.html#wildcards",
    "href": "unix_cheat_sheet.html#wildcards",
    "title": "Unix Cheat Sheet",
    "section": "Wildcards",
    "text": "Wildcards\n\n\n\n\n\n\n\n*\nmatch any number of characters\n\n\n?\nmatch any character only once\n\n\nExamples\n\n\n\nls sample*\nlist all files that start with the word “sample”\n\n\nls *.txt\nlist all the files with .txt extension\n\n\ncp * {another/directory}\ncopy all the files in the current directory to a different directory"
  },
  {
    "objectID": "unix_cheat_sheet.html#redirect-output",
    "href": "unix_cheat_sheet.html#redirect-output",
    "title": "Unix Cheat Sheet",
    "section": "Redirect Output",
    "text": "Redirect Output\n\n\n\n\n\n\n\n{command} > {file}\nredirect output to a file (overwrites if the file exists)\n\n\n{command} >> {file}\nappend output to a file (creates a new file if it does not already exist)"
  },
  {
    "objectID": "unix_cheat_sheet.html#combining-commands-with-pipes",
    "href": "unix_cheat_sheet.html#combining-commands-with-pipes",
    "title": "Unix Cheat Sheet",
    "section": "Combining Commands with | Pipes",
    "text": "Combining Commands with | Pipes\n\n\n\n\n\n\n\n<command1> | <command2>\nthe output of “command1” is passed as input to “command2”\n\n\nExamples\n\n\n\nls | wc -l\ncount the number of files in a directory\n\n\ncat {file1} {file2} | less\nconcatenate files and view them with less\n\n\ncat {file} | grep \"{pattern}\" | wc -l\ncount how many lines in the file have a match with “pattern”"
  },
  {
    "objectID": "unix_cheat_sheet.html#credit",
    "href": "unix_cheat_sheet.html#credit",
    "title": "Unix Cheat Sheet",
    "section": "Credit",
    "text": "Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/cambiotraining/hpc-intro"
  }
]